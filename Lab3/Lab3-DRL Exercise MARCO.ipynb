{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
    "\n",
    "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
    "\n",
    "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
    "\n",
    "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest) \n",
    "\n",
    "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
    "\n",
    "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
    "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
    "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# Plus one non standard one -- we need this to sample from policies.\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an environment, observation, and policy, sample from pi(a | obs). Returns the\n",
    "# selected action and the log probability of that action (needed for policy gradient).\n",
    "def select_action(env, obs, policy):\n",
    "    dist = Categorical(policy(obs))\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return (action.item(), log_prob.reshape(1))\n",
    "\n",
    "\n",
    "def select_action2(env, obs, policy):\n",
    "    dist = Categorical(policy(obs))\n",
    "    action = dist.sample()\n",
    "    return (action.item())\n",
    "\n",
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array. There's probably a better way to do this.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "\n",
    "# Given an environment and a policy, run it up to the maximum number of steps.\n",
    "def run_episode(env, policy, maxlen=500):\n",
    "    # Collect just about everything.\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset the environment and start the episode.\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        # Get the current observation, run the policy and select an action.\n",
    "        env.render()\n",
    "        obs = torch.tensor(obs)\n",
    "        (action, log_prob) = select_action(env, obs, policy)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # Advance the episode by executing the selected action.\n",
    "        (obs, reward, term, trunc, info) = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions, torch.cat(log_probs), rewards)\n",
    "\n",
    "\n",
    "def run_episode2(env, policy, maxlen=500):\n",
    "    # Collect just about everything.\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset the environment and start the episode.\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        # Get the current observation, run the policy and select an action.\n",
    "        env.render()\n",
    "        obs = torch.tensor(obs)\n",
    "        action = select_action(env, obs, policy)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Advance the episode by executing the selected action.\n",
    "        (obs, reward, term, trunc, info) = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions,  rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple, but generic, policy network with one hidden layer.\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env, inner=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], inner)\n",
    "        self.fc2 = nn.Linear(inner, env.action_space.n)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.softmax(self.fc2(s), dim=-1)\n",
    "        return s\n",
    "\n",
    "# A simple, but generic, policy network with one hidden layer.\n",
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self, env, inner=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], inner)\n",
    "        self.fc2 = nn.Linear(inner, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = self.fc2(s)\n",
    "        return s\n",
    "    \n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(*args)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A direct, inefficient, and probably buggy of the REINFORCE policy gradient algorithm.\n",
    "def reinforce(policy, N,  env, env_render=None, gamma=0.99, num_episodes=10, baseline=None, display=False, comment='', mini_batch=50):\n",
    "    # The only non-vanilla part: we use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    target_policy = policy\n",
    "    # If we have a baseline network, create the optimizer.\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        opt_baseline = torch.optim.Adam(baseline.parameters(), lr=1e-3)\n",
    "        baseline.train()\n",
    "        print('Training agent with baseline value network.')\n",
    "    elif baseline == 'std':\n",
    "        print('Training agent with standardization baseline.')\n",
    "    else:\n",
    "        print('Training agent with no baseline.')\n",
    "        \n",
    "    # Track episode rewards in a list.\n",
    "    running_rewards = [0.0]\n",
    "    \n",
    "    # The main training loop.\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        #replay_buffer = []\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        v = run_episode(env, target_policy)\n",
    "        if len(v)<mini_batch:\n",
    "            (observations, actions, log_probs, rewards) = v\n",
    "        else:\n",
    "            (observations, actions, log_probs, rewards) = random.sample(v, mini_batch)\n",
    "        #replay_buffer.append((observations, actions, log_probs, rewards))\n",
    "        # Compute the discounted reward for every step of the episode.\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        #v = random.sample(replay_buffer,mini_batch)\n",
    "            \n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "        \n",
    "        # Handle baseline.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            with torch.no_grad():\n",
    "                target = returns - baseline(torch.stack(observations))\n",
    "        elif baseline == 'std':\n",
    "            target = (returns - returns.mean()) / returns.std()\n",
    "        else:\n",
    "            target = returns\n",
    "        \n",
    "        # Make an optimization step\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Update policy network\n",
    "        loss = (-log_probs * target).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # Update baseline network.\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            opt_baseline.zero_grad()\n",
    "            loss_baseline = ((returns - baseline(torch.stack(observations)))**2.0).mean()\n",
    "            loss_baseline.backward()\n",
    "            opt_baseline.step()\n",
    "        \n",
    "        # Render an episode after every 100 policy updates.\n",
    "        if not episode % 100:\n",
    "            breakpoint()\n",
    "            if display:\n",
    "                policy.eval()\n",
    "                (obs, _, _, _) = run_episode(env_render, policy)\n",
    "                policy.train()\n",
    "            print(f'Running reward: {running_rewards[-1]}')\n",
    "        \n",
    "        if episode % N:\n",
    "            target_policy= policy\n",
    "    \n",
    "    # Return the running rewards.\n",
    "    policy.eval()\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        baseline.eval()\n",
    "    return running_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'namedtuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m Transition \u001b[38;5;241m=\u001b[39m \u001b[43mnamedtuple\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransition\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m                         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_state\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReplayMemory\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, capacity):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'namedtuple' is not defined"
     ]
    }
   ],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "def deep_q(policy, N, env, mini_batch=50, env_render=None, gamma=0.99, num_episodes=10, baseline=None, display=False, comment='' ):\n",
    "    memory = ReplayMemory(1000)\n",
    "    target_policy = policy\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    running_rewards = [0.0]\n",
    "    \n",
    "    # The main training loop.\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        #replay_buffer = []\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        v = run_episode(env, target_policy)\n",
    "        if len(v)<mini_batch:\n",
    "            (observations, actions, log_probs, rewards) = v\n",
    "        else:\n",
    "            (observations, actions, log_probs, rewards) = random.sample(v, mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with baseline value network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward: 0.6062709331512451\n",
      "Running reward: 19.528285438505787\n",
      "Running reward: 18.281430065204972\n",
      "Running reward: 23.114441148449057\n",
      "Running reward: 26.018947080721684\n",
      "Running reward: 30.525440354496688\n",
      "Running reward: 34.0358021252995\n",
      "Running reward: 40.12696732834528\n",
      "Running reward: 48.74539913128046\n",
      "Running reward: 65.23168265911778\n",
      "Running reward: 76.86276865859509\n",
      "Running reward: 81.78377447164769\n",
      "Running reward: 87.4362799196845\n",
      "Running reward: 91.8410402786324\n",
      "Running reward: 91.29235293632021\n",
      "Running reward: 93.68388085093584\n",
      "Running reward: 93.5447103717694\n",
      "Running reward: 93.82018143208828\n",
      "Running reward: 96.7060883375681\n",
      "Running reward: 97.02602085932713\n",
      "Running reward: 97.63135496178529\n",
      "Running reward: 96.49007017146998\n",
      "Running reward: 96.94322435372526\n",
      "Running reward: 97.92727622384311\n",
      "Running reward: 97.06383321966184\n",
      "Running reward: 95.92390348060621\n",
      "Running reward: 97.92924915973701\n",
      "Running reward: 95.53633423141304\n",
      "Running reward: 97.72296440553532\n",
      "Running reward: 96.85743788503159\n",
      "Running reward: 93.55980901575995\n",
      "Running reward: 95.45704912298785\n",
      "Running reward: 96.11861784724944\n",
      "Running reward: 97.6445717744408\n",
      "Running reward: 96.84120354491047\n",
      "Running reward: 97.7301117002921\n",
      "Running reward: 98.0306973357593\n",
      "Running reward: 98.09941614708688\n",
      "Running reward: 97.67482361716432\n",
      "Running reward: 98.29918913911597\n",
      "Running reward: 98.348170449278\n",
      "Running reward: 98.21862382352599\n",
      "Running reward: 98.28278651847725\n",
      "Running reward: 98.34913032185592\n",
      "Running reward: 98.26369905222957\n",
      "Running reward: 98.08680438586516\n",
      "Running reward: 98.32239040641589\n",
      "Running reward: 98.3493647978317\n",
      "Running reward: 98.34952450050426\n",
      "Running reward: 98.33589139003294\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env_render = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env, inner=16)\n",
    "baseline = BaselineNet(env, inner=16)\n",
    "\n",
    "# Train the agent.\n",
    "plt.plot(reinforce(policy,50, env, env_render, num_episodes=5000, gamma=0.99, baseline=baseline, comment='-NO-BASELINE-INNER=16', mini_batch=100))\n",
    "\n",
    "# Close up everything\n",
    "env_render.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_render = gym.make('CartPole-v1', render_mode='human')\n",
    "for _ in range(5):\n",
    "    run_episode(env_render, policy)\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"CartPole-v1\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In output c'è un valore di Q per ogni azione, n_observation è la dimensione dello spazio degli stati\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size: is the number of transitions sampled from the replay buffer\n",
    "# gamma: is the discount factor as mentioned in the previous section\n",
    "# eps_start: is the starting value of epsilon\n",
    "# eps_end: is the final value of epsilon\n",
    "# eps_decay: controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# tau: is the update rate of the target network\n",
    "# lr: is the learning rate of the ``AdamW`` optimizer\n",
    "\n",
    "batch_size = 32 #128 for CartPole\n",
    "gamma = 0.99\n",
    "eps_start = 0.9\n",
    "eps_end = 0.05\n",
    "eps_decay = 1000\n",
    "tau = 0.005\n",
    "lr = 0.0005\n",
    "update_target = 10\n",
    "update_freq =4\n",
    "memory_size = 200000\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "#Queste reti restituiscono il Q value, non i logits delle azioni\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_end + (eps_start - eps_end) * \\\n",
    "        math.exp(-1. * steps_done / eps_decay)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227.1351306396008 episode:0\n",
      "-150.2605508771141 episode:1\n",
      "-308.4428502131635 episode:2\n",
      "-348.07133502623606 episode:3\n",
      "-367.14948969391924 episode:4\n",
      "-373.8029962767593 episode:5\n",
      "-354.32258316818974 episode:6\n",
      "-334.659762779487 episode:7\n",
      "-303.96532375438215 episode:8\n",
      "-305.3642416261464 episode:9\n",
      "-305.875544752594 episode:10\n",
      "-285.2510505772864 episode:11\n",
      "-270.8432167646652 episode:12\n",
      "-261.68209580118935 episode:13\n",
      "-247.50514418028712 episode:14\n",
      "-219.6328540206344 episode:15\n",
      "-198.31365959559741 episode:16\n",
      "-189.18573228434778 episode:17\n",
      "-187.61946483513614 episode:18\n",
      "-182.21171632771615 episode:19\n",
      "-178.5924493169618 episode:20\n",
      "-175.972787140051 episode:21\n",
      "-171.03965862222344 episode:22\n",
      "-165.14850278813338 episode:23\n",
      "-158.59090211944456 episode:24\n",
      "-154.01260969415935 episode:25\n",
      "-153.58437715939988 episode:26\n",
      "-150.46835458130604 episode:27\n",
      "-150.93116326210594 episode:28\n",
      "-147.16022742111429 episode:29\n",
      "-146.31620426709404 episode:30\n",
      "-142.23727007086273 episode:31\n",
      "-140.85500364224762 episode:32\n",
      "-143.30541209302746 episode:33\n",
      "-142.50845473151966 episode:34\n",
      "-141.07894849547282 episode:35\n",
      "-138.6731138581074 episode:36\n",
      "-136.55365743007832 episode:37\n",
      "-138.80432956028739 episode:38\n",
      "-139.76918179770254 episode:39\n",
      "-139.1757253068266 episode:40\n",
      "-140.6723562416304 episode:41\n",
      "-142.97554304377124 episode:42\n",
      "-142.06781350062576 episode:43\n",
      "-140.13581555948193 episode:44\n",
      "-139.68865866989086 episode:45\n",
      "-137.6746600019424 episode:46\n",
      "-136.66175555961013 episode:47\n",
      "-134.79994996553503 episode:48\n",
      "-133.1833470185357 episode:49\n",
      "-132.11693603186507 episode:50\n",
      "-138.15534722201667 episode:51\n",
      "-131.086403866266 episode:52\n",
      "-120.89999448473294 episode:53\n",
      "-115.40121244012096 episode:54\n",
      "-107.87132282646243 episode:55\n",
      "-105.054707315893 episode:56\n",
      "-101.89852192804847 episode:57\n",
      "-100.12787305598046 episode:58\n",
      "-102.6180248394256 episode:59\n",
      "-99.95494905506388 episode:60\n",
      "-99.36717142540759 episode:61\n",
      "-97.93183218578586 episode:62\n",
      "-114.79503462214552 episode:63\n",
      "-114.79138644566476 episode:64\n",
      "-129.3056242795523 episode:65\n",
      "-142.91220216149682 episode:66\n",
      "-198.72845549064996 episode:67\n",
      "-203.8795195631286 episode:68\n",
      "-207.18479628681737 episode:69\n",
      "-206.90936289115035 episode:70\n",
      "-207.1847414148276 episode:71\n",
      "-207.6602585165599 episode:72\n",
      "-207.3955424083965 episode:73\n",
      "-205.70314055317706 episode:74\n",
      "-206.03033181814993 episode:75\n",
      "-207.80180624841122 episode:76\n",
      "-208.89878084478676 episode:77\n",
      "-209.16441729147596 episode:78\n",
      "-208.1257838007836 episode:79\n",
      "-208.8579968908976 episode:80\n",
      "-207.0498916819626 episode:81\n",
      "-206.7459712285587 episode:82\n",
      "-204.11805657532196 episode:83\n",
      "-204.2578955888799 episode:84\n",
      "-202.42322530543663 episode:85\n",
      "-198.96297870644355 episode:86\n",
      "-198.49646527897858 episode:87\n",
      "-197.82269310284914 episode:88\n",
      "-196.7649157656715 episode:89\n",
      "-198.14109957959997 episode:90\n",
      "-196.12998158107587 episode:91\n",
      "-192.27800260849654 episode:92\n",
      "-191.99037325521797 episode:93\n",
      "-189.16779251318403 episode:94\n",
      "-186.1408182383171 episode:95\n",
      "-185.8089273662868 episode:96\n",
      "-185.40568913430366 episode:97\n",
      "-184.29511482533104 episode:98\n",
      "-184.42220348294074 episode:99\n",
      "-182.72975338950323 episode:100\n",
      "-183.03032680198422 episode:101\n",
      "-181.66587050845789 episode:102\n",
      "-182.11944224894916 episode:103\n",
      "-180.22296714953293 episode:104\n",
      "-178.09469432846453 episode:105\n",
      "-175.92490799843057 episode:106\n",
      "-173.54640411754838 episode:107\n",
      "-170.0550999329211 episode:108\n",
      "-167.5558950625677 episode:109\n",
      "-166.6559611684887 episode:110\n",
      "-164.67620238370068 episode:111\n",
      "-161.38175519211487 episode:112\n",
      "-158.10515162052397 episode:113\n",
      "-147.74093394722914 episode:114\n",
      "-138.595995415382 episode:115\n",
      "-129.47428476557775 episode:116\n",
      "-114.777748888875 episode:117\n",
      "-81.73123633476585 episode:118\n",
      "-54.90152490575694 episode:119\n",
      "-53.440697070929 episode:120\n",
      "-51.78772758998447 episode:121\n",
      "-49.997434794906766 episode:122\n",
      "-49.288431351101615 episode:123\n",
      "-49.37759995573868 episode:124\n",
      "-48.26779957316774 episode:125\n",
      "-48.17811890622859 episode:126\n",
      "-47.35002303812518 episode:127\n",
      "-47.773806248607656 episode:128\n",
      "-45.77843613333318 episode:129\n",
      "-45.112521408673956 episode:130\n",
      "-44.7252315892991 episode:131\n",
      "-44.80805499111124 episode:132\n",
      "-46.149766920393716 episode:133\n",
      "-46.1101126853174 episode:134\n",
      "-47.0315072851715 episode:135\n",
      "-47.998828505890195 episode:136\n",
      "-47.55569926485689 episode:137\n",
      "-48.416469574510266 episode:138\n",
      "-50.293531584111136 episode:139\n",
      "-49.592158724827605 episode:140\n",
      "-48.86564089842341 episode:141\n",
      "-51.254504854004445 episode:142\n",
      "-51.83933982425821 episode:143\n",
      "-51.89474483513033 episode:144\n",
      "-51.30703211707425 episode:145\n",
      "-51.60623933960891 episode:146\n",
      "-51.689119175947056 episode:147\n",
      "-51.59899255105067 episode:148\n",
      "-52.79385361505796 episode:149\n",
      "-54.86987556884755 episode:150\n",
      "-56.54471550429042 episode:151\n",
      "-57.01896441095638 episode:152\n",
      "-56.400699808645356 episode:153\n",
      "-58.61152839464677 episode:154\n",
      "-59.686012089780625 episode:155\n",
      "-61.07816546548831 episode:156\n",
      "-62.92878681795079 episode:157\n",
      "-64.27750753248966 episode:158\n",
      "-64.96577974695533 episode:159\n",
      "-65.8931185479436 episode:160\n",
      "-65.89876126725227 episode:161\n",
      "-67.51419596386253 episode:162\n",
      "-68.51585634867031 episode:163\n",
      "-70.20501224904469 episode:164\n",
      "-70.81412478649874 episode:165\n",
      "-71.62451426125617 episode:166\n",
      "-71.16668038998348 episode:167\n",
      "-72.28350921491524 episode:168\n",
      "-72.7599926131098 episode:169\n",
      "-71.49801365377138 episode:170\n",
      "-72.47415840688603 episode:171\n",
      "-73.83055361614483 episode:172\n",
      "-74.4207335854225 episode:173\n",
      "-76.0034615592769 episode:174\n",
      "-76.25353870782841 episode:175\n",
      "-75.75961525272312 episode:176\n",
      "-75.05804761645214 episode:177\n",
      "-74.82812819790743 episode:178\n",
      "-75.20842627905871 episode:179\n",
      "-75.66912021239328 episode:180\n",
      "-76.79813424449279 episode:181\n",
      "-77.84381198993032 episode:182\n",
      "-78.44686965578768 episode:183\n",
      "-79.68289333696261 episode:184\n",
      "-78.95773313067978 episode:185\n",
      "-78.28636787755642 episode:186\n",
      "-76.34779494962616 episode:187\n",
      "-75.36881836621149 episode:188\n",
      "-75.21010902780549 episode:189\n",
      "-75.00496565578254 episode:190\n",
      "-74.99752337123192 episode:191\n",
      "-75.99031726011252 episode:192\n",
      "-75.70886898031821 episode:193\n",
      "-75.80260387479862 episode:194\n",
      "-75.14055948491038 episode:195\n",
      "-73.39365445063476 episode:196\n",
      "-72.5270131286468 episode:197\n",
      "-73.59843119087957 episode:198\n",
      "-72.43165699792857 episode:199\n",
      "-71.64836290312607 episode:200\n",
      "-71.40821025232043 episode:201\n",
      "-70.99621625264678 episode:202\n",
      "-70.56158780522178 episode:203\n",
      "-69.03189074188515 episode:204\n",
      "-68.39502783104524 episode:205\n",
      "-67.34978811010967 episode:206\n",
      "-66.65580645847804 episode:207\n",
      "-67.19056096184218 episode:208\n",
      "-65.54284816561054 episode:209\n",
      "-63.740833461337516 episode:210\n",
      "-62.86790870827775 episode:211\n",
      "-63.013680891183114 episode:212\n",
      "-62.152237765600276 episode:213\n",
      "-60.68687113485762 episode:214\n",
      "-60.2512427992878 episode:215\n",
      "-59.596600168947646 episode:216\n",
      "-59.30994959191039 episode:217\n",
      "-59.15027270628605 episode:218\n",
      "-59.611583582630104 episode:219\n",
      "-58.696970801928074 episode:220\n",
      "-59.144034218645054 episode:221\n",
      "-57.581306073957656 episode:222\n",
      "-54.725826843171596 episode:223\n",
      "-52.246952391350206 episode:224\n",
      "-51.03199486919055 episode:225\n",
      "-51.71051613824673 episode:226\n",
      "-53.278974122932816 episode:227\n",
      "-53.57529802486751 episode:228\n",
      "-52.79477651025136 episode:229\n",
      "-53.63968679832787 episode:230\n",
      "-51.46824085088001 episode:231\n",
      "-49.36788482123773 episode:232\n",
      "-48.34163524614958 episode:233\n",
      "-48.74555332434174 episode:234\n",
      "-46.727886374741786 episode:235\n",
      "-47.2784602361577 episode:236\n",
      "-46.60661670682937 episode:237\n",
      "-48.26722010163872 episode:238\n",
      "-47.13498106505611 episode:239\n",
      "-46.13415093722999 episode:240\n",
      "-45.57065510791549 episode:241\n",
      "-43.786218529772114 episode:242\n",
      "-40.204823120057846 episode:243\n",
      "-38.95310747838001 episode:244\n",
      "-38.04616854024017 episode:245\n",
      "-37.33226461553504 episode:246\n",
      "-36.830669431964935 episode:247\n",
      "-36.390639408556986 episode:248\n",
      "-35.90531204957835 episode:249\n",
      "-36.57412522371166 episode:250\n",
      "-30.386762072317023 episode:251\n",
      "-29.660121630660406 episode:252\n",
      "-28.800633601273713 episode:253\n",
      "-27.736740171840214 episode:254\n",
      "-26.629656835726706 episode:255\n",
      "-26.743968223797193 episode:256\n",
      "-26.04176275533448 episode:257\n",
      "-25.371266725594314 episode:258\n",
      "-25.900541000801145 episode:259\n",
      "-24.300691412813514 episode:260\n",
      "-23.732656792643382 episode:261\n",
      "-24.061826611770453 episode:262\n",
      "-17.908412930359017 episode:263\n",
      "-15.953090671601847 episode:264\n",
      "-17.135486239569914 episode:265\n",
      "-16.309461040178633 episode:266\n",
      "-14.976052363184849 episode:267\n",
      "-13.824350571607283 episode:268\n",
      "-11.95449799088589 episode:269\n",
      "-10.377944673814604 episode:270\n",
      "-9.355917513981204 episode:271\n",
      "-7.315197497333743 episode:272\n",
      "-1.9394512745824886 episode:273\n",
      "-1.5988020986370362 episode:274\n",
      "-0.17481884910149276 episode:275\n",
      "1.7902923290060118 episode:276\n",
      "7.821250833949843 episode:277\n",
      "8.125178907989188 episode:278\n",
      "9.790060155332082 episode:279\n",
      "10.944310244357252 episode:280\n",
      "10.873622594627864 episode:281\n",
      "15.480160694472442 episode:282\n",
      "16.7257243351052 episode:283\n",
      "18.745956834846556 episode:284\n",
      "26.48348480778277 episode:285\n",
      "28.099324378573048 episode:286\n",
      "32.829719640601354 episode:287\n",
      "34.31793689281266 episode:288\n",
      "38.714123652141225 episode:289\n",
      "43.303855160235514 episode:290\n",
      "48.080902647654426 episode:291\n",
      "55.258486355133876 episode:292\n",
      "58.22576772068172 episode:293\n",
      "59.8121968217462 episode:294\n",
      "59.97670558099944 episode:295\n",
      "63.02271943329801 episode:296\n",
      "63.21853804493398 episode:297\n",
      "68.56493113061995 episode:298\n",
      "73.93425410257238 episode:299\n",
      "74.89881605652457 episode:300\n",
      "79.16479518249196 episode:301\n",
      "79.38656289255823 episode:302\n",
      "79.02010990284155 episode:303\n",
      "78.8144465776173 episode:304\n",
      "79.61649476637226 episode:305\n",
      "81.76368001779304 episode:306\n",
      "82.16889854624768 episode:307\n",
      "83.10283092508442 episode:308\n",
      "88.20483229242662 episode:309\n",
      "92.83280640607678 episode:310\n",
      "92.82251777311347 episode:311\n",
      "93.23382926710674 episode:312\n",
      "97.91412522416617 episode:313\n",
      "97.84719735314276 episode:314\n",
      "104.12760789525635 episode:315\n",
      "107.55389203060858 episode:316\n",
      "107.94421326121619 episode:317\n",
      "108.47273551880153 episode:318\n",
      "106.70604268842045 episode:319\n",
      "109.36456868055933 episode:320\n",
      "106.44906521858995 episode:321\n",
      "107.0300051697587 episode:322\n",
      "110.6018064819316 episode:323\n",
      "112.78192786545283 episode:324\n",
      "114.90423026505682 episode:325\n",
      "113.90860662490915 episode:326\n",
      "109.60967306847807 episode:327\n",
      "106.33053667467345 episode:328\n",
      "106.0213803080199 episode:329\n",
      "106.23608902594529 episode:330\n",
      "110.60782559358931 episode:331\n",
      "111.67194702168531 episode:332\n",
      "112.92115507662679 episode:333\n",
      "111.16813887153192 episode:334\n",
      "112.13111765488026 episode:335\n",
      "109.2656257908462 episode:336\n",
      "110.43003708733795 episode:337\n",
      "108.25195713997942 episode:338\n",
      "108.86517436180412 episode:339\n",
      "107.503279328356 episode:340\n",
      "106.64700717969987 episode:341\n",
      "107.24709358897391 episode:342\n",
      "107.49700886855896 episode:343\n",
      "109.38922813871437 episode:344\n",
      "112.09708858461062 episode:345\n",
      "111.82305256991648 episode:346\n",
      "110.93965278601519 episode:347\n",
      "109.64863346837262 episode:348\n",
      "107.27346709637736 episode:349\n",
      "105.13938635247372 episode:350\n",
      "99.6719388164049 episode:351\n",
      "96.78881710100696 episode:352\n",
      "91.33686793440798 episode:353\n",
      "89.66135710675383 episode:354\n",
      "89.68317041249847 episode:355\n",
      "87.93534244489642 episode:356\n",
      "84.79338563542828 episode:357\n",
      "82.28321862306392 episode:358\n",
      "76.58619184406328 episode:359\n",
      "77.48237874922263 episode:360\n",
      "76.68739573999237 episode:361\n",
      "72.10773645597502 episode:362\n",
      "72.12714284891011 episode:363\n",
      "68.72257652431759 episode:364\n",
      "66.13922221358038 episode:365\n",
      "62.810413134233066 episode:366\n",
      "61.356655999308344 episode:367\n",
      "55.276494838323586 episode:368\n",
      "52.219639442075376 episode:369\n",
      "51.05459081904227 episode:370\n",
      "48.3389003993438 episode:371\n",
      "45.66418048568272 episode:372\n",
      "44.15548335293577 episode:373\n",
      "40.22300864808788 episode:374\n",
      "33.92832413464906 episode:375\n",
      "29.218624267607648 episode:376\n",
      "26.578014237983197 episode:377\n",
      "25.477181531589604 episode:378\n",
      "24.102023035112207 episode:379\n",
      "25.458703169039783 episode:380\n",
      "24.8696517528019 episode:381\n",
      "19.904329406572813 episode:382\n",
      "12.104832846135668 episode:383\n",
      "11.621623545578498 episode:384\n",
      "8.102738227317042 episode:385\n",
      "4.94868342192847 episode:386\n",
      "2.6333500585716543 episode:387\n",
      "4.744711479779987 episode:388\n",
      "1.8963193578151192 episode:389\n",
      "4.1898623069056615 episode:390\n",
      "-0.9972834392635772 episode:391\n",
      "-4.907885753536773 episode:392\n",
      "-4.859118978343124 episode:393\n",
      "-4.5054283883827635 episode:394\n",
      "-11.024869098102737 episode:395\n",
      "-13.159170022547551 episode:396\n",
      "-16.65920798911928 episode:397\n",
      "-17.769671226769177 episode:398\n",
      "-17.63904898703138 episode:399\n",
      "-16.28433454489612 episode:400\n",
      "-11.00531572279783 episode:401\n",
      "-6.4985245159680884 episode:402\n",
      "-7.378776367979165 episode:403\n",
      "-7.366148152584457 episode:404\n",
      "-6.9844399682450105 episode:405\n",
      "-7.673280649015623 episode:406\n",
      "-7.523241680420085 episode:407\n",
      "-6.987095404214034 episode:408\n",
      "-6.180674903907016 episode:409\n",
      "-5.764560362627797 episode:410\n",
      "-5.082717598600926 episode:411\n",
      "-4.515014306161275 episode:412\n",
      "-3.968382691800232 episode:413\n",
      "-3.8014723004122817 episode:414\n",
      "-3.6725907273405887 episode:415\n",
      "-6.464120744019441 episode:416\n",
      "-8.678947878991936 episode:417\n",
      "-10.826600165056332 episode:418\n",
      "-13.021751861690808 episode:419\n",
      "-11.905957130955223 episode:420\n",
      "-11.233888790824478 episode:421\n",
      "-11.758085601092377 episode:422\n",
      "-11.862731588303477 episode:423\n",
      "-10.980645367518427 episode:424\n",
      "-10.502628011585468 episode:425\n",
      "-10.037952684640553 episode:426\n",
      "-9.692234498667501 episode:427\n",
      "-9.14462709838818 episode:428\n",
      "-9.373262103391717 episode:429\n",
      "-11.243632729344247 episode:430\n",
      "-13.445859943454668 episode:431\n",
      "-13.112258261676423 episode:432\n",
      "-12.144423211016644 episode:433\n",
      "-11.492382940295661 episode:434\n",
      "-11.44994043853794 episode:435\n",
      "-11.694946262188488 episode:436\n",
      "-11.522268206059577 episode:437\n",
      "-11.951199861277523 episode:438\n",
      "-12.311009693877729 episode:439\n",
      "-12.336130748734236 episode:440\n",
      "-11.744445271519453 episode:441\n",
      "-9.749529005702847 episode:442\n",
      "-10.29078007304106 episode:443\n",
      "-10.85007593258721 episode:444\n",
      "-10.683941162855545 episode:445\n",
      "-11.799504483415529 episode:446\n",
      "-12.18522436007198 episode:447\n",
      "-12.957639232840902 episode:448\n",
      "-13.454584123554623 episode:449\n",
      "-13.28212796229501 episode:450\n",
      "-12.907356125093134 episode:451\n",
      "-6.844855924132512 episode:452\n",
      "-8.499358363126449 episode:453\n",
      "-10.068235958500981 episode:454\n",
      "-10.279360305971238 episode:455\n",
      "-4.260817671856922 episode:456\n",
      "2.0896248369148713 episode:457\n",
      "7.381424655167983 episode:458\n",
      "9.824356335607176 episode:459\n",
      "9.973838064816913 episode:460\n",
      "10.558944106329925 episode:461\n",
      "15.308990522504377 episode:462\n",
      "14.897029112225395 episode:463\n",
      "15.599305443938347 episode:464\n",
      "19.003545449049042 episode:465\n",
      "16.248842535503794 episode:466\n",
      "14.06265344568877 episode:467\n",
      "16.251848345316507 episode:468\n",
      "17.212509737577392 episode:469\n",
      "16.919160379461015 episode:470\n",
      "16.703575293707832 episode:471\n",
      "16.2143689622416 episode:472\n",
      "13.125311952776752 episode:473\n",
      "15.552542833554606 episode:474\n",
      "16.398455503511386 episode:475\n",
      "19.482508029748825 episode:476\n",
      "20.370413629546945 episode:477\n",
      "27.846854919332927 episode:478\n",
      "34.28723813980463 episode:479\n",
      "38.35772891086361 episode:480\n",
      "36.675100768314586 episode:481\n",
      "36.53560305528734 episode:482\n",
      "41.58488483276662 episode:483\n",
      "41.76809864881166 episode:484\n",
      "47.176712866908254 episode:485\n",
      "51.00312873377057 episode:486\n",
      "56.65281967580743 episode:487\n",
      "60.91366796320387 episode:488\n",
      "65.68716635629231 episode:489\n",
      "71.4040378128341 episode:490\n",
      "76.00427016821814 episode:491\n",
      "79.71910939445952 episode:492\n",
      "84.894118769189 episode:493\n",
      "89.57030525695616 episode:494\n",
      "94.34510737531946 episode:495\n",
      "98.83173188828158 episode:496\n",
      "105.22972489451661 episode:497\n",
      "110.95313364685242 episode:498\n",
      "115.75054293484529 episode:499\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "#running_rewards=[0.0]\n",
    "average_score=deque(maxlen=100)\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state, and initialize score for lunar_lander episode\n",
    "    score = 0\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    #discounted_reward = 0\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        #discounted_reward += GAMMA**(t+1)*reward\n",
    "        score += reward\n",
    "        \n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            #running_rewards.append(0.05 * discounted_reward + 0.95 * running_rewards[-1])\n",
    "            average_score.append(score)\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "    \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        if i_episode%update_freq:\n",
    "            optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        #Update target network every N episodes\n",
    "        #if i_episode% update_target :\n",
    "        #    target_net_state_dict = target_net.state_dict()\n",
    "        #    policy_net_state_dict = policy_net.state_dict()\n",
    "        #    for key in policy_net_state_dict:\n",
    "        #        target_net_state_dict[key] = policy_net_state_dict[key]\n",
    "\n",
    "        if done:\n",
    "            #episode_durations.append(t + 1)\n",
    "            average_score.append(score)\n",
    "            #running_rewards.append(0.05 * discounted_reward + 0.95 * running_rewards[-1])\n",
    "            #plot_durations()\n",
    "            break\n",
    "    print(np.average(average_score),f'episode:{i_episode}')\n",
    "\n",
    "#print('Complete')\n",
    "#plot_durations(show_result=True)\n",
    "#plt.ioff()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "b=0\n",
    "for _ in range(600):\n",
    "    \n",
    "    action = select_action(state)\n",
    "    observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        b+=1\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    else:\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "print(b)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest) \n",
    "\n",
    "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
    "\n",
    "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
    "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
    "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.make(\"CarRacing-v2\", continuous= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Functions to take RGB image to grayscale and to put the 3 frames in place of the RGB channels\n",
    "\n",
    "def process_state_image(state):\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "    state = state.astype(float)\n",
    "    state /= 255.0\n",
    "    return state\n",
    "\n",
    "def generate_state_frame_stack_from_queue(deque):\n",
    "    frame_stack = np.array(deque)\n",
    "    # Move stack dimension to the channel dimension (stack, x, y) -> (x, y, stack)\n",
    "    return np.transpose(frame_stack, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "\n",
    "gamma = 0.95\n",
    "memory_size =5000\n",
    "epsilon_decay= 0.9999\n",
    "epsilon_min = 0.1\n",
    "learning_rate= 0.001\n",
    "\n",
    "memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNA_car(nn.Module):\n",
    "    def __init__(self, env, frame_stack_num = 3):\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "                    nn.Conv2d(frame_stack_num,6, kernel_size=7, stride= 3),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=2),\n",
    "                    nn.Conv2d(6,12, kernel_size= 4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=2),\n",
    "                    nn.Flatten(),\n",
    "                    nn.LazyLinear(216),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(216, env.action_space.n)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model= DQNA_car()\n",
    "target_model=model\n",
    "model.to(device)\n",
    "target_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take actions based on epsilon greedy\n",
    "def act(env, model, state, epsilon):\n",
    "        if np.random.rand() > epsilon:\n",
    "            act_values = model(state)   #.predict(np.expand_dims(state, axis=0))\n",
    "            action_index = np.argmax(act_values[0])\n",
    "        else:\n",
    "            action_index = random.randrange(env.action_space.n)\n",
    "        return env.action_space[action_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(model, batch_size, memory, epochs):\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        train_state = []\n",
    "        train_target = []\n",
    "        for state, action_index, reward, next_state, done in minibatch:\n",
    "            target = model(np.expand_dims(state, axis=0))[0]\n",
    "            if done:\n",
    "                target[action_index] = reward\n",
    "            else:\n",
    "                t = target_model(np.expand_dims(next_state, axis=0))[0]\n",
    "                target[action_index] = reward + gamma * np.max(t)\n",
    "            train_state.append(state)\n",
    "            train_target.append(target)\n",
    "        \n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        inputs = np.array(train_state).to(device)\n",
    "        targets = np.array(train_target).to(device)\n",
    "        output = model(inputs)\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "\n",
    "\n",
    "        model.fit(np.array(train_state), np.array(train_target), epochs=1, verbose=0)\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER                        = True\n",
    "STARTING_EPISODE              = 1\n",
    "ENDING_EPISODE                = 1000\n",
    "SKIP_FRAMES                   = 2\n",
    "TRAINING_BATCH_SIZE           = 64\n",
    "SAVE_TRAINING_FREQUENCY       = 25\n",
    "UPDATE_TARGET_MODEL_FREQUENCY = 5\n",
    "frame_stack_num               = 3\n",
    "epsilon                       = 1\n",
    "memory                        = deque(maxlen=memory_size)\n",
    "optimizer                     = optim.AdamW(policy_net.parameters(), lr=lr, eps=1e-7)\n",
    "\n",
    "#memory = ReplayMemory(memory_size)\n",
    "\n",
    "for e in range(STARTING_EPISODE, ENDING_EPISODE+1):\n",
    "        init_state = env.reset()\n",
    "        init_state = process_state_image(init_state)\n",
    "\n",
    "        total_reward = 0\n",
    "        negative_reward_counter = 0\n",
    "        state_frame_stack_queue = deque([init_state]*frame_stack_num, maxlen=frame_stack_num)\n",
    "        time_frame_counter = 1\n",
    "        done = False\n",
    "\n",
    "        while True:\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "\n",
    "            current_state_frame_stack = generate_state_frame_stack_from_queue(state_frame_stack_queue)\n",
    "            action = act(current_state_frame_stack, model, epsilon)\n",
    "\n",
    "            reward = 0\n",
    "            for _ in range(SKIP_FRAMES+1):\n",
    "                next_state, r, done, info = env.step(action)\n",
    "                reward += r\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # If continually getting negative reward 10 times after the tolerance steps, terminate this episode\n",
    "            negative_reward_counter = negative_reward_counter + 1 if time_frame_counter > 100 and reward < 0 else 0\n",
    "\n",
    "            # Extra bonus for the model if it uses full gas\n",
    "            if action[1] == 1 and action[2] == 0:\n",
    "                reward *= 1.5\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state = process_state_image(next_state)\n",
    "            state_frame_stack_queue.append(next_state)\n",
    "            next_state_frame_stack = generate_state_frame_stack_from_queue(state_frame_stack_queue)\n",
    "\n",
    "            #agent.memorize(current_state_frame_stack, action, reward, next_state_frame_stack, done)\n",
    "            memory.append((state, env.action_space.index(action), reward, next_state, done))\n",
    "            \n",
    "\n",
    "            if done or negative_reward_counter >= 25 or total_reward < 0:\n",
    "                print('Episode: {}/{}, Scores(Time Frames): {}, Total Rewards(adjusted): {:.2}, Epsilon: {:.2}'.format(e, ENDING_EPISODE, time_frame_counter, float(total_reward), float(epsilon)))\n",
    "                break\n",
    "            if len(memory) > TRAINING_BATCH_SIZE:\n",
    "                agent.replay(TRAINING_BATCH_SIZE)\n",
    "                minibatch = random.sample(memory, TRAINING_BATCH_SIZE)\n",
    "            time_frame_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "        if e % UPDATE_TARGET_MODEL_FREQUENCY == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        if e % SAVE_TRAINING_FREQUENCY == 0:\n",
    "            model.save('./save/trial_{}.h5'.format(e))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinf_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
