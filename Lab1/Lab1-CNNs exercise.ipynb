{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "## Exercise 1: Warming Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2b6d1-3df0-464c-9a5f-8c611257a971",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a8282-2322-4dca-b76e-2f3863bc75fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:57:20.045237Z",
     "start_time": "2024-04-28T11:57:20.026034Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with some standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import wandb\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc12cc-8422-47bf-8d8e-0950ac05ae96",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "Here is some basic dataset loading, validation splitting code to get you started working with MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a69db-0416-444a-9be4-5f055ff48bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:57:20.126097Z",
     "start_time": "2024-04-28T11:57:20.073420Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard MNIST transform.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST train and test.\n",
    "ds_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "ds_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train into train and validation.\n",
    "val_size = 5000\n",
    "I = np.random.permutation(len(ds_train))\n",
    "ds_val = Subset(ds_train, I[:val_size])\n",
    "ds_train = Subset(ds_train, I[val_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e05e96-7707-4490-98b8-50cb5e330af1",
   "metadata": {},
   "source": [
    "#### Boilerplate training and evaluation code\n",
    "\n",
    "This is some **very** rough training, evaluation, and plotting code. Again, just to get you started. I will be *very* disappointed if any of this code makes it into your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005d3f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:57:20.136293Z",
     "start_time": "2024-04-28T11:57:20.126097Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to train a model for a single epoch over the data loader.\n",
    "def train_epoch(model, dl, opt, epoch='Unknown', device='cpu'):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for (xs, ys) in tqdm(dl, desc=f'Training epoch {epoch}', leave=True):\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "        # Zero out the gradients.\n",
    "        opt.zero_grad()\n",
    "        # Forward through the model.\n",
    "        logits = model(xs) \n",
    "        # Compute the cross-entropy loss.\n",
    "        loss = F.cross_entropy(logits, ys)\n",
    "        # Backward through the model.\n",
    "        loss.backward()\n",
    "        # Update the model parameters.\n",
    "        opt.step()\n",
    "        # Save the loss value.\n",
    "        losses.append(loss.item())\n",
    "    # Return the average loss for this epoch.\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Function to evaluate model over all samples in the data loader.\n",
    "def evaluate_model(model, dl, device='cpu'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    with torch.no_grad():\n",
    "        for (xs, ys) in tqdm(dl, desc='Evaluating', leave=False):\n",
    "            xs = xs.to(device)\n",
    "            logits = model(xs)\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            \n",
    "            # Save the ground truth and predictions.\n",
    "            ground_truths.append(ys)\n",
    "            predictions.append(preds.detach().cpu().numpy())\n",
    "            \n",
    "    predictions = np.hstack(predictions)\n",
    "    ground_truths = np.hstack(ground_truths)\n",
    "    \n",
    "    # Return accuracy score and classification report.\n",
    "    return accuracy_score(ground_truths, predictions), classification_report(ground_truths, predictions, zero_division=0, digits=3)\n",
    "\n",
    "# Simple function to plot the loss curve and validation accuracy.\n",
    "def plot_validation_curves(training_history):\n",
    "    losses, accuracies = zip(*training_history)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Average Training Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracies)\n",
    "    plt.title(f'Best Accuracy = {np.max(accuracies)} @ epoch {np.argmax(accuracies)}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84b1167357748c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:57:20.143467Z",
     "start_time": "2024-04-28T11:57:20.136293Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dl_train, dl_val, opt, epochs, model_name, dataset_type, lr, batch_size, device='cpu'):\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"DLA Assigment 1\",\n",
    "        name=model_name + \"-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"architecture\": model_name,\n",
    "            \"dataset\": dataset_type,\n",
    "            \"epochs\": epochs,\n",
    "            \"learning_rate\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"device\": device,\n",
    "            \"optimizer\": \"Adam\"\n",
    "        }\n",
    "    )\n",
    "    losses_and_accs = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(model, dl_train, opt, epoch, device=device)\n",
    "        (val_acc, _) = evaluate_model(model, dl_val, device=device)\n",
    "        losses_and_accs.append((loss, val_acc))\n",
    "        \n",
    "        print(f'Epoch {epoch}: Loss - {loss:.4f}, Validation Acc - {val_acc:.4f}')\n",
    "        # wandb\n",
    "        wandb.log({\"acc\": val_acc, \"loss\": loss})\n",
    "                \n",
    "    # [optional] finish the wandb run, necessary in notebooks\n",
    "    wandb.finish()    \n",
    "\n",
    "    torch.save(model.state_dict(), f\"model_states/model_{model_name}.pt\")\n",
    "\n",
    "    return losses_and_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875008c3-306c-4e39-a845-d7bda7862621",
   "metadata": {},
   "source": [
    "#### A basic, parameterized MLP\n",
    "\n",
    "This is a very basic implementation of a Multilayer Perceptron. Don't waste too much time trying to figure out how it works -- the important detail is that it allows you to pass in a list of input, hidden layer, and output *widths*. **Your** implementation should also support this for the exercises to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e503a-37df-4fb9-94e7-85d0adb494bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:57:20.153290Z",
     "start_time": "2024-04-28T11:57:20.143467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(nin, nout) for (nin, nout) in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return reduce(lambda f, g: lambda x: g(F.relu(f(x))), self.layers, lambda x: x.flatten(1))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A non-parametric CNN.\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, num=1, channels=8, size=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Conv2d(channels, channels, kernel_size=size, padding='same') for _ in range(num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return reduce(lambda f, g: lambda x: g(F.relu(f(x))), self.layers, lambda x: x)(x)\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num=2, channels=8, size=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, channels, kernel_size=size)\n",
    "        self.cblock1 = ConvBlock(num=num, channels=channels, size=size)\n",
    "        self.cblock2 = ConvBlock(num=num, channels=channels, size=size)\n",
    "        self.cblock3 = ConvBlock(num=num, channels=channels, size=size)\n",
    "        self.output = nn.Linear(channels, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.cblock1(x)\n",
    "        x = F.max_pool2d(x, 3, 2)\n",
    "        x = self.cblock2(x)\n",
    "        x = F.max_pool2d(x, 3, 2)\n",
    "        x = self.cblock3(x)\n",
    "        x = F.avg_pool2d(x, kernel_size=x.shape[2:]).squeeze()\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06e26-8fa3-414e-a502-8d1c18ba9eb7",
   "metadata": {},
   "source": [
    "#### A *very* minimal training pipeline.\n",
    "\n",
    "Here is some basic training and evaluation code to get you started.\n",
    "\n",
    "**Important**: I cannot stress enough that this is a **terrible** example of how to implement a training pipeline. You can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91afcace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:58:01.850495Z",
     "start_time": "2024-04-28T11:57:59.500597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture hyperparameters.\n",
    "input_size = 28*28\n",
    "width = 16\n",
    "depth = 2\n",
    "\n",
    "# Dataloaders.\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=4)\n",
    "dl_val   = torch.utils.data.DataLoader(ds_val, batch_size, num_workers=4)\n",
    "dl_test  = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Instantiate model and optimizer.\n",
    "model_mlp = MLP([input_size] + [width]*depth + [10]).to(device)\n",
    "opt = torch.optim.Adam(params=model_mlp.parameters(), lr=lr)\n",
    "\n",
    "# Training \n",
    "losses_and_accs = train_model(model_mlp, dl_train, dl_val, opt, epochs, \"MLP\", \"MNIST\", lr, batch_size, device=device)\n",
    "\n",
    "# And finally plot the curves.\n",
    "plot_validation_curves(losses_and_accs)\n",
    "print(f'Accuracy report on TEST:\\n {evaluate_model(model_mlp, dl_test, device=device)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df",
   "metadata": {},
   "source": [
    "### Exercise 1.1: A baseline MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d96405-36e7-4074-803c-fb02576cd528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(28*28, 128),  # First narrow layer\n",
    "            nn.Linear(128, 128),  # Second narrow layer\n",
    "            nn.Linear(128, 64),  # Third narrow layer\n",
    "            nn.Linear(64, 64), # Fourth narrow layer\n",
    "            nn.Linear(64, 10)  # Output layer\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2869de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device {device}')\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "batch_size = 128\n",
    "\n",
    "# Dataloaders.\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=4)\n",
    "dl_val   = torch.utils.data.DataLoader(ds_val, batch_size, num_workers=4)\n",
    "dl_test  = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Instantiate model and optimizer.\n",
    "model_mlp = SimpleMLP().to(device)\n",
    "print(model_mlp.layers)\n",
    "opt = torch.optim.Adam(params=model_mlp.parameters(), lr=lr)\n",
    "\n",
    "# Training\n",
    "losses_and_accs = train_model(model_mlp, dl_train, dl_val, opt, epochs, \"SimpleMLP\", \"MNIST\", lr, batch_size, device=device)\n",
    "\n",
    "# And finally plot the curves.\n",
    "plot_validation_curves(losses_and_accs)\n",
    "print(f'Accuracy report on TEST:\\n {evaluate_model(model_mlp, dl_test, device=device)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8ad9b-e3ae-4c49-9bec-35aaea149b08",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Rinse and Repeat\n",
    "\n",
    "Repeat the verification you did above, but with **Convolutional** Neural Networks. If you were careful about abstracting your model and training code, this should be a simple exercise. Show that **deeper** CNNs *without* residual connections do not always work better and **even deeper** ones *with* residual connections.\n",
    "\n",
    "**Hint**: You probably should do this exercise using CIFAR10, since MNIST is *very* easy (at least up to about 99% accuracy).\n",
    "\n",
    "**Spoiler**: If you plan to do optional exercise 2.3, you should think *very* carefully about the architectures of your CNNs here (so you can reuse them!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23e58797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the basic building blocks of ResNet: Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "# Define the ResNet model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_blocks, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64    \n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(self.in_channels)\n",
    "\n",
    "        self.layer1 = self._make_layer(ResidualBlock, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(ResidualBlock, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(ResidualBlock, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(ResidualBlock, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)    # [stride, 1, 1, ...] (num_blocks times) ensures that the first block has stride=stride\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv1(x)))    # [batch_size, 3, 32, 32] -> [batch_size, 64, 32, 32]\n",
    "        out = self.layer1(out)                     # [batch_size, 64, 32, 32] -> [batch_size, 64, 32, 32] (stride=1)\n",
    "        out = self.layer2(out)                     # [batch_size, 64, 32, 32] -> [batch_size, 128, 16, 16] (stride=2)\n",
    "        out = self.layer3(out)                     # [batch_size, 128, 16, 16] -> [batch_size, 256, 8, 8] (stride=2)\n",
    "        out = self.layer4(out)                     # [batch_size, 256, 8, 8] -> [batch_size, 512, 4, 4] (stride=2)\n",
    "        out = F.avg_pool2d(out, 4)                   # [batch_size, 512, 4, 4] -> [batch_size, 512, 1, 1] (avg_pool2d)\n",
    "        out = out.view(out.size(0), -1)            # [batch_size, 512, 1, 1] -> [batch_size, 512]\n",
    "        out = self.fc(out)                         # [batch_size, 512] -> [batch_size, num_classes]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train into train and validation.\n",
    "val_size = 5000\n",
    "I = np.random.permutation(len(trainset))\n",
    "ds_val = Subset(trainset, I[:val_size])\n",
    "ds_train = Subset(trainset, I[val_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4646fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=batch_size, num_workers=2)\n",
    "dl_test = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CNN model\n",
    "CNN = CNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(CNN.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training CNN\n",
    "losses_and_accs = train_model(CNN, dl_train, dl_val, optimizer, epochs, \"CNN\", \"CIFAR10\", learning_rate, batch_size, device=device)\n",
    "\n",
    "# And finally plot the curves.\n",
    "plot_validation_curves(losses_and_accs)\n",
    "print(f'Accuracy report on TEST:\\n {evaluate_model(CNN, dl_test, device=device)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d22aaac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pnkajlro) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3390923d3f034cc798a571916f2f701e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.015 MB uploaded\\r'), FloatProgress(value=0.07799479166666666, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet18-20240429-195916</strong> at: <a href='https://wandb.ai/lorebaia/DLA%20Assigment%201/runs/pnkajlro/workspace' target=\"_blank\">https://wandb.ai/lorebaia/DLA%20Assigment%201/runs/pnkajlro/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240429_195916-pnkajlro\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pnkajlro). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c0857c05e04ba5be851e3884e62f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\loreb\\Documents\\IntellijProjects\\PycharmProjects\\DLA\\Lab1\\wandb\\run-20240429_201003-u52ccoq5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lorebaia/DLA%20Assigment%201/runs/u52ccoq5/workspace' target=\"_blank\">ResNet18-20240429-201003</a></strong> to <a href='https://wandb.ai/lorebaia/DLA%20Assigment%201' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lorebaia/DLA%20Assigment%201' target=\"_blank\">https://wandb.ai/lorebaia/DLA%20Assigment%201</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lorebaia/DLA%20Assigment%201/runs/u52ccoq5/workspace' target=\"_blank\">https://wandb.ai/lorebaia/DLA%20Assigment%201/runs/u52ccoq5/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 4/391 [00:19<30:50,  4.78s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(ResNet18\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Training ResCNN\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m losses_and_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mResNet18\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResNet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCIFAR10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# And finally plot the curves.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m plot_validation_curves(losses_and_accs)\n",
      "Cell \u001b[1;32mIn[14], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dl_train, dl_val, opt, epochs, model_name, dataset_type, lr, batch_size, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m losses_and_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     (val_acc, _) \u001b[38;5;241m=\u001b[39m evaluate_model(model, dl_val, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     21\u001b[0m     losses_and_accs\u001b[38;5;241m.\u001b[39mappend((loss, val_acc))\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dl, opt, epoch, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, ys)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Backward through the model.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Update the model parameters.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the ResCNN model\n",
    "ResNet18 = ResNet18(num_blocks=[2,2,2,2], num_classes=10).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(ResNet18.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training ResCNN\n",
    "losses_and_accs = train_model(ResNet18, dl_train, dl_val, optimizer, epochs, \"ResNet18\", \"CIFAR10\", learning_rate, batch_size, device=device)\n",
    "\n",
    "# And finally plot the curves.\n",
    "plot_validation_curves(losses_and_accs)\n",
    "print(f'Accuracy report on TEST:\\n {evaluate_model(ResNet18, dl_test, device=device)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Below are **three** exercises that ask you to deepen your understanding of Deep Networks for visual recognition. You must choose **at least one** of the below for your final submission -- feel free to do **more**, but at least **ONE** you must submit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07978e8e-9f2e-4949-9699-495af6cb6349",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Explain why Residual Connections are so effective\n",
    "Use your two models (with and without residual connections) you developed above to study and **quantify** why the residual versions of the networks learn more effectively.\n",
    "\n",
    "**Hint**: A good starting point might be looking at the gradient magnitudes passing through the networks during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_magnitudes(model):\n",
    "    gradient_magnitudes = []\n",
    "    for param in model.parameters():\n",
    "        gradient_magnitudes.append(param.grad.abs().mean().item())\n",
    "    return gradient_magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e81a3-08ca-4549-a2f8-f47cf5a0308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate gradient magnitudes for CNN and ResCNN\n",
    "cnn_gradients = gradient_magnitudes(model_cnn)\n",
    "residual_gradients = gradient_magnitudes(model_rescnn)\n",
    "\n",
    "# Print mean gradient magnitudes\n",
    "print(\"Mean Gradient Magnitudes - CNN:\")\n",
    "print(sum(cnn_gradients) / len(cnn_gradients))\n",
    "\n",
    "print(\"Mean Gradient Magnitudes - Residual CNN:\")\n",
    "print(sum(residual_gradients) / len(residual_gradients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a3a7b-2ed6-4f58-a1b7-5ab1fc432893",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Fully-convolutionalize a network.\n",
    "Take one of your trained classifiers and **fully-convolutionalize** it. That is, turn it into a network that can predict classification outputs at *all* pixels in an input image. Can you turn this into a **detector** of handwritten digits? Give it a try.\n",
    "\n",
    "**Hint 1**: Sometimes the process of fully-convolutionalization is called \"network surgery\".\n",
    "\n",
    "**Hint 2**: To test your fully-convolutionalized networks you might want to write some functions to take random MNIST samples and embed them into a larger image (i.e. in a regular grid or at random positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33c912-0716-44ef-a91b-47ca19a2b2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A non-parametric CNN.\n",
    "class ResConvBlock(nn.Module):\n",
    "    def __init__(self, num=1, channels=8, size=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Conv2d(channels, channels, kernel_size=size, padding=(size-1)//2) for _ in range(num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            res = x\n",
    "            x = F.relu(layer(x) + res)\n",
    "        return x\n",
    "\n",
    "class FullyResCNN(nn.Module):\n",
    "    def __init__(self, num=2, channels=8, size=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, channels, kernel_size=size, padding=(size - 1) // 2)\n",
    "        self.cblock1 = ConvBlock(num=num, channels=channels, size=size)\n",
    "        self.cblock2 = ConvBlock(num=num, channels=channels, size=size)\n",
    "        self.cblock3 = ConvBlock(num=num, channels=channels, size=size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.cblock1(x)\n",
    "        x = F.max_pool2d(x, 3, 2)\n",
    "        x = self.cblock2(x)\n",
    "        x = F.max_pool2d(x, 3, 2)\n",
    "        x = self.cblock3(x)\n",
    "        x = F.avg_pool2d(x, kernel_size=x.shape[2:]).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a0413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8243f811-8227-4c6f-b07f-56e8cd91643a",
   "metadata": {},
   "source": [
    "### Exercise 2.3: *Explain* the predictions of a CNN\n",
    "\n",
    "Use the CNN model you trained in Exercise 1.2 and implement [*Class Activation Maps*](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n",
    "\n",
    "> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "\n",
    "Use your implementation to demonstrate how your trained CNN *attends* to specific image features to recognize *specific* classes.\n",
    "\n",
    "**Note**: Feel free to implement [Grad-CAM](https://arxiv.org/abs/1610.02391) instead of CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634a700-56c2-48fd-96e0-4c94d1bd0cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
