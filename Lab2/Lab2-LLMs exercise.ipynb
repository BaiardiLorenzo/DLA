{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "809f8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd466d3b-cc41-4de3-9f82-3547569909f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "class Dante:\n",
    "    \"\"\"A class that aggregates functionality related to the \"corpus\" used.\"\"\"\n",
    "    def __init__(self, train = True, train_size=0.9, block_size=128):\n",
    "        self._block_size = block_size\n",
    "        self._train = train\n",
    "\n",
    "        #Load entier text file\n",
    "        with open('commedia.txt', 'r', encoding='utf-8') as fd:\n",
    "            rawdata = fd.read()\n",
    "\n",
    "        # Extract tokend BEFORE splitting. Our tokens are characters.\n",
    "        self._tokens = sorted(set(rawdata))\n",
    "        self.num_tokens = len(self._tokens)\n",
    "\n",
    "        # Select train or val/test set.\n",
    "        rawdata = rawdata[:int(len(rawdata)*train_size)] if train else rawdata[int(len(rawdata)*train_size):]\n",
    "\n",
    "        # Build the encode/decode dictionaries mapping chars to token ids and back.\n",
    "        self._c2i = {c: i for (i, c) in enumerate(self._tokens)}\n",
    "        self._i2c = {i: c for (i, c) in enumerate(self._tokens)}\n",
    "\n",
    "        # Encode \n",
    "        self.encode = lambda s: [self._c2i[c] for c in s] # encoder: take a string, output a list of integers\n",
    "        self.decode = lambda l: ''.join([self._i2c[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "        # Encode the data\n",
    "        self._data = torch.tensor(self.encode(rawdata), dtype=torch.long)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        \"\"\" Retrives a random batch of context and targets.\"\"\"\n",
    "        ix = torch.randint(len(self._data) - self._block_size, (batch_size,))\n",
    "        print(self._data)\n",
    "        x = torch.stack([self._data[i:i+self._block_size] for i in ix])\n",
    "        y = torch.stack([self._data[i+1:i+self._block_size+1] for i in ix])\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data) - self._block_size - 1\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        xs = self._data[i:i+self._block_size]\n",
    "        ys = self._data[i+1:i+self._block_size+1]\n",
    "        return (xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f6e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dante(train=True, train_size=0.9, block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd83c799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75, 66, 73, 1, 74, 66, 87, 87, 76, 1, 65, 66, 73, 1, 64, 62, 74, 74, 70, 75]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode a string\n",
    "ds.encode('nel mezzo del cammin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f95abb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nel mezzo del cammin'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode an Encoded string -> return 'nel mezzo del cammin'\n",
    "ds.decode(ds.encode('nel mezzo del cammin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f637104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([51, 69, 66,  ..., 81, 79, 76])\n"
     ]
    }
   ],
   "source": [
    "(xs, ys) = ds.get_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac55f640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128]), torch.Size([32, 128]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaa13d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([70, 62, 11,  1, 73, 66, 81, 81, 76, 79, 11,  1, 81, 70,  1, 68, 70, 82,\n",
       "        79, 76, 11,  0,  1,  1, 80,  7, 66, 73, 73, 66,  1, 75, 76, 75,  1, 80,\n",
       "        70, 66, 75,  1, 65, 70,  1, 73, 82, 75, 68, 62,  1, 68, 79, 62, 87, 70,\n",
       "        62,  1, 83, 76, 81, 66, 11,  0,  0, 64, 69,  7, 70,  7,  1, 83, 70, 65,\n",
       "        70,  1, 77, 66, 79,  1, 78, 82, 66, 73, 73,  7, 62, 66, 79, 66,  1, 68,\n",
       "        79, 76, 80, 80, 76,  1, 66,  1, 80, 64, 82, 79, 76,  0,  1,  1, 83, 66,\n",
       "        75, 70, 79,  1, 75, 76, 81, 62, 75, 65, 76,  1, 82, 75, 62,  1, 67, 70,\n",
       "        68, 82])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"First input\"\n",
    "xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d62f333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ia, lettor, ti giuro,\\n  s'elle non sien di lunga grazia vote,\\n\\nch'i' vidi per quell'aere grosso e scuro\\n  venir notando una figu\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds.decode(xs[0]) Not working\n",
    "ds.decode(xs[0].numpy()) # Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc21504b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a, lettor, ti giuro,\\n  s'elle non sien di lunga grazia vote,\\n\\nch'i' vidi per quell'aere grosso e scuro\\n  venir notando una figur\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.decode(ys[0].numpy()) # Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "986c204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All configuration parameters for out Transformer\n",
    "block_size = 128\n",
    "train_size = 0.9\n",
    "batch_size = 32\n",
    "n_embed = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af5d1abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11,  0,  1,  ..., 59,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate datasets for training and test\n",
    "ds_train = Dante(train=True, train_size=train_size, block_size=block_size)\n",
    "ds_test = Dante(train=False, train_size=train_size, block_size=block_size)\n",
    "(xs, ys) = ds_test.get_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dedf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top-level GPT nn.Module\n",
    "class GTPLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed):\n",
    "        super().__init__()\n",
    "        self._vocab_size = vocab_size\n",
    "        self._n_embd = n_embed\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        (B, T) = idx.shape\n",
    "        tok_emb = self.token_embedding(idx) # (B, T, C)\n",
    "        return tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce065322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTPLanguageModel(vocab_size=ds_train.num_tokens, n_embed = n_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c798bf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0182,  0.6876,  0.9524, -0.1810,  0.9837, -0.6089, -1.1648,  0.2752,\n",
       "        -0.6810,  0.4400,  1.3942, -0.2972,  0.2556,  1.7411, -0.1625,  0.7471,\n",
       "        -0.3994,  0.6829,  0.6663, -1.9936, -1.0045,  0.6590,  1.0105, -0.0707,\n",
       "         1.5947,  0.0098,  0.7688, -0.8266, -0.4158,  1.1425, -0.6613,  0.3734,\n",
       "        -0.3173, -0.1288,  1.8279, -0.1044,  1.3437,  1.6375,  1.3891,  0.1766,\n",
       "        -1.1703,  0.6529,  0.9052,  0.4542,  0.8510,  0.0475, -1.1846,  0.7598,\n",
       "         1.0428,  1.2485, -0.1313, -0.1652,  0.0153, -0.1453,  0.8056,  0.1221,\n",
       "        -1.8702, -0.1466,  0.7614,  0.3381, -0.6846, -1.0877,  1.8149, -0.5938,\n",
       "        -0.3843,  1.2736,  1.1190, -0.9846,  0.2179, -0.1396,  0.3629,  0.3197,\n",
       "         0.8835, -0.4273, -0.9002,  0.1076, -1.4472,  0.2919, -1.0444, -0.3461,\n",
       "         0.9479,  0.7831, -1.8522,  1.7290,  2.5879,  0.3881,  0.2460,  0.6543,\n",
       "         0.6894,  1.1303,  0.3790, -0.6986, -1.5515,  0.2599, -0.7662, -2.3683,\n",
       "         0.2489, -0.9762,  0.9289,  0.8374,  0.1356,  2.2834,  0.4203, -0.5832,\n",
       "        -1.1883, -1.4646,  0.1313, -0.1989,  0.3273, -0.8213, -1.7832, -1.8499,\n",
       "        -0.9608,  0.5331,  1.4874,  1.5569,  1.0515, -1.4750, -0.4488, -1.7029,\n",
       "        -1.2705, -0.9757,  0.0098,  1.2787, -0.0545, -1.0491, -1.0499,  0.1849],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xs)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\huggingface_hub-0.23.0-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   45,   417,   502, 47802,  1619, 12172,  1084,  2566, 18216,   430,\n",
      "           410,  5350]])\n",
      "tensor([[   34, 13481, 21504,   442,  1789,    78, 34898]])\n",
      "tensor([[28875, 14057, 14266,   952]])\n",
      "tensor([[   35, 12427,   435,   394, 29864]])\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "# Load key classes GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "print(tokenizer(\"Nel mezzo del cammin di nostra vita\", return_tensors='pt')[\"input_ids\"])\n",
    "print(tokenizer(\"Ciao mi chiamo Dante\", return_tensors='pt')[\"input_ids\"])\n",
    "print(tokenizer(\"Paolo Brosio\", return_tensors='pt')[\"input_ids\"])\n",
    "print(tokenizer(\"Dante alighieri\", return_tensors='pt')[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nel mezzo del cammin di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "# Load key classes GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Generate text from a prompt\n",
    "prompt = \"Nel mezzo del cammin di nostra vita\"\n",
    "generated = model.generate(tokenizer(prompt, return_tensors='pt')[\"input_ids\"], max_length=100)\n",
    "# print(generated)\n",
    "print(tokenizer.decode(generated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58081878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nel mezzo del cammin di nostra vita faggio alla e di siguella, una di giorale della santo e una perche di che di sanna.\n",
      "\n",
      "D'autre pela susere quelequando sugliando il sable l'apicher che l'ampli sopentor pitta di spagna, neque l'argento e sebata e che qu\n"
     ]
    }
   ],
   "source": [
    "# Look at the do_sample and temperature parameters\n",
    "generated = model.generate(tokenizer(prompt, return_tensors='pt')[\"input_ids\"], max_length=100, do_sample=True, temperature=0.9)   \n",
    "# print(generated)\n",
    "print(tokenizer.decode(generated[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "# Exercise 3.1: Training a Text Classifier (easy)\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model.\n",
    "\n",
    "# Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898bc094",
   "metadata": {},
   "source": [
    "## Exercise 3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90c03c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2f71473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a model for a single epoch over the data loader.\n",
    "def train_epoch(model, dl, opt, epoch='Unknown', device='cpu'):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for (xs, mas, ys) in tqdm(dl, desc=f'Training epoch {epoch}', leave=True):\n",
    "        xs, mas, ys = xs.to(device), mas.to(device), ys.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xs, mas)\n",
    "        loss = F.cross_entropy(logits, ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Function to evaluate model over all samples in the data loader.\n",
    "def evaluate_model(model, dl, device='cpu'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    with torch.no_grad():\n",
    "        for (xs, mas, ys) in tqdm(dl, desc='Evaluating', leave=False):\n",
    "            xs = xs.to(device)\n",
    "            mas = mas.to(device)\n",
    "            logits = model(xs, mas)\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            ground_truths.append(ys)\n",
    "            predictions.append(preds.detach().cpu().numpy())\n",
    "    predictions = np.hstack(predictions)\n",
    "    ground_truths = np.hstack(ground_truths)\n",
    "    return accuracy_score(ground_truths, predictions), classification_report(ground_truths, predictions, zero_division=0, digits=3, output_dict=True)\n",
    "\n",
    "def train_model(model, dl_train, dl_val, opt, epochs, model_name, dataset_type, lr, batch_size, device='cpu'):\n",
    "    wandb.init(\n",
    "        project=\"DLA Assigment 2\",\n",
    "        name=model_name + \"-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"architecture\": model_name,\n",
    "            \"dataset\": dataset_type,\n",
    "            \"epochs\": epochs,\n",
    "            \"learning_rate\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"device\": device,\n",
    "            \"optimizer\": \"AdamW\"\n",
    "        }\n",
    "    )\n",
    "    # wandb.watch(model, nn.CrossEntropyLoss, log=\"all\", log_freq=10)\n",
    "    losses_and_accs = []\n",
    "    classification_report = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(model, dl_train, opt, epoch, device=device)\n",
    "        (val_acc, class_rep) = evaluate_model(model, dl_val, device=device)\n",
    "        losses_and_accs.append((loss, val_acc))\n",
    "        classification_report.append(class_rep)\n",
    "        \n",
    "        print(f'Epoch {epoch}: Loss - {loss:.4f}, Validation Acc - {val_acc:.4f}')\n",
    "        # wandb\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": loss, \"acc\": val_acc, \"classification_report\": class_rep})\n",
    "                \n",
    "    # wandb.unwatch(model)\n",
    "    # [optional] finish the wandb run, necessary in notebooks\n",
    "    wandb.finish()    \n",
    "\n",
    "    # torch.save(model.state_dict(), f\"model_states/model_{model_name}.pt\")\n",
    "    # torch.save(model, f\"model/model_{model_name}.pt\")\n",
    "    return losses_and_accs\n",
    "\n",
    "\n",
    "# Simple function to plot the loss curve and validation accuracy.\n",
    "def plot_validation_curves(training_history):\n",
    "    losses, accuracies = zip(*training_history)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Average Training Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracies)\n",
    "    plt.title(f'Best Accuracy = {np.max(accuracies)} @ epoch {np.argmax(accuracies)}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cbe3d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, model, n_classes):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(model.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.model(x, attention_mask=mask).last_hidden_state[:, 0]\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5f8754b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loreb\\AppData\\Local\\Temp\\ipykernel_6524\\4047413839.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(xx['input_ids'])\n",
      "C:\\Users\\loreb\\AppData\\Local\\Temp\\ipykernel_6524\\4047413839.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.tensor(xx['attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# Convert train, val, and test to DataLoader\n",
    "def get_dataloader(dataset, batch_size):\n",
    "    # Tokenize only the text column and return pytorch tensors\n",
    "    xx = tokenizer(dataset['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    x = torch.tensor(xx['input_ids'])\n",
    "    a = torch.tensor(xx['attention_mask'])\n",
    "    y = torch.tensor(dataset['label'])\n",
    "    dataset = torch.utils.data.TensorDataset(x, a, y)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "\n",
    "dl_train = get_dataloader(dataset['train'], batch_size)\n",
    "dl_val = get_dataloader(dataset['validation'], batch_size)\n",
    "dl_test = get_dataloader(dataset['test'], batch_size)\n",
    "\n",
    "n_classes = len(set(dataset['train']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "56372c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\huggingface_hub-0.23.0-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the DistilBERT model\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Instantiate the model\n",
    "model = DistilBERTClassifier(bert, n_classes).to(device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "lr = 1e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6cdd9f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:51031zrv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DistilBERT-20240503-185516</strong> at: <a href='https://wandb.ai/lorebaia/DLA%20Assigment%202/runs/51031zrv/workspace' target=\"_blank\">https://wandb.ai/lorebaia/DLA%20Assigment%202/runs/51031zrv/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240503_185516-51031zrv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:51031zrv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\loreb\\Documents\\IntellijProjects\\PycharmProjects\\DLA\\Lab2\\wandb\\run-20240503_193555-g8r0n13r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lorebaia/DLA%20Assigment%202/runs/g8r0n13r/workspace' target=\"_blank\">DistilBERT-20240503-193555</a></strong> to <a href='https://wandb.ai/lorebaia/DLA%20Assigment%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lorebaia/DLA%20Assigment%202' target=\"_blank\">https://wandb.ai/lorebaia/DLA%20Assigment%202</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lorebaia/DLA%20Assigment%202/runs/g8r0n13r/workspace' target=\"_blank\">https://wandb.ai/lorebaia/DLA%20Assigment%202/runs/g8r0n13r/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 0/176 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDistilBERT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweet_eval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plot_validation_curves(results)\n",
      "Cell \u001b[1;32mIn[114], line 51\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dl_train, dl_val, opt, epochs, model_name, dataset_type, lr, batch_size, device)\u001b[0m\n\u001b[0;32m     49\u001b[0m classification_report \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 51\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     (val_acc, class_rep) \u001b[38;5;241m=\u001b[39m evaluate_model(model, dl_val, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     53\u001b[0m     losses_and_accs\u001b[38;5;241m.\u001b[39mappend((loss, val_acc))\n",
      "Cell \u001b[1;32mIn[114], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dl, opt, epoch, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(xs, mas)\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, ys)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "results = train_model(classifier, dl_train, dl_val, optimizer, epochs, \"DistilBERT\", \"tweet_eval\", lr, batch_size, device=device)\n",
    "\n",
    "# Plot the results\n",
    "plot_validation_curves(results)\n",
    "print(f'Accuracy report on TEST:\\n {evaluate_model(model, dl_test, device=device)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fa324dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\huggingface_hub-0.23.0-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 2.9835972785949707, Acc 0.125, Mean Loss 2.9835972785949707, Batch 0 of 45000\n",
      "Epoch 0, Loss 2.9624533653259277, Acc 0.125, Mean Loss 2.973025321960449, Batch 256 of 45000\n",
      "Epoch 0, Loss 2.9558889865875244, Acc 0.0625, Mean Loss 2.967313210169474, Batch 512 of 45000\n",
      "Epoch 0, Loss 2.9042811393737793, Acc 0.0625, Mean Loss 2.9515551924705505, Batch 768 of 45000\n",
      "Epoch 0, Loss 2.9294204711914062, Acc 0.0625, Mean Loss 2.947128248214722, Batch 1024 of 45000\n",
      "Epoch 0, Loss 2.8204762935638428, Acc 0.125, Mean Loss 2.9260195891062417, Batch 1280 of 45000\n",
      "Epoch 0, Loss 3.019408702850342, Acc 0.03125, Mean Loss 2.939360891069685, Batch 1536 of 45000\n",
      "Epoch 0, Loss 2.9148595333099365, Acc 0.25, Mean Loss 2.936298221349716, Batch 1792 of 45000\n",
      "Epoch 0, Loss 2.9502100944519043, Acc 0.0625, Mean Loss 2.937843985027737, Batch 2048 of 45000\n",
      "Epoch 0, Loss 2.8904333114624023, Acc 0.15625, Mean Loss 2.933102917671204, Batch 2304 of 45000\n",
      "Epoch 0, Loss 2.833491325378418, Acc 0.21875, Mean Loss 2.9240473183718594, Batch 2560 of 45000\n",
      "Epoch 0, Loss 2.818894386291504, Acc 0.15625, Mean Loss 2.91528457403183, Batch 2816 of 45000\n",
      "Epoch 0, Loss 2.6839683055877686, Acc 0.34375, Mean Loss 2.8974910149207482, Batch 3072 of 45000\n",
      "Epoch 0, Loss 2.6782498359680176, Acc 0.3125, Mean Loss 2.881830930709839, Batch 3328 of 45000\n",
      "Epoch 0, Loss 2.884071111679077, Acc 0.1875, Mean Loss 2.881980276107788, Batch 3584 of 45000\n",
      "Epoch 0, Loss 2.7351813316345215, Acc 0.1875, Mean Loss 2.872805342078209, Batch 3840 of 45000\n",
      "Epoch 0, Loss 2.852015256881714, Acc 0.1875, Mean Loss 2.87158239589018, Batch 4096 of 45000\n",
      "Epoch 0, Loss 2.8907268047332764, Acc 0.125, Mean Loss 2.8726459741592407, Batch 4352 of 45000\n",
      "Epoch 0, Loss 2.840101957321167, Acc 0.15625, Mean Loss 2.8709331311677633, Batch 4608 of 45000\n",
      "Epoch 0, Loss 2.650540351867676, Acc 0.21875, Mean Loss 2.8599134922027587, Batch 4864 of 45000\n",
      "Epoch 0, Loss 2.9814727306365967, Acc 0.0625, Mean Loss 2.8657020273662748, Batch 5120 of 45000\n",
      "Epoch 0, Loss 2.6366662979125977, Acc 0.21875, Mean Loss 2.855291312391108, Batch 5376 of 45000\n",
      "Epoch 0, Loss 2.7226288318634033, Acc 0.28125, Mean Loss 2.8495233784551206, Batch 5632 of 45000\n",
      "Epoch 0, Loss 2.744209051132202, Acc 0.15625, Mean Loss 2.845135281483332, Batch 5888 of 45000\n",
      "Epoch 0, Loss 2.7615244388580322, Acc 0.0625, Mean Loss 2.8417908477783205, Batch 6144 of 45000\n",
      "Epoch 0, Loss 2.934081792831421, Acc 0.125, Mean Loss 2.8453404995111318, Batch 6400 of 45000\n",
      "Epoch 0, Loss 2.784219741821289, Acc 0.125, Mean Loss 2.8430767677448414, Batch 6656 of 45000\n",
      "Epoch 0, Loss 2.89990234375, Acc 0.1875, Mean Loss 2.8451062526021684, Batch 6912 of 45000\n",
      "Epoch 0, Loss 2.907259702682495, Acc 0.125, Mean Loss 2.8472494750187316, Batch 7168 of 45000\n",
      "Epoch 0, Loss 2.782701253890991, Acc 0.15625, Mean Loss 2.845097867647807, Batch 7424 of 45000\n",
      "Epoch 0, Loss 2.876495122909546, Acc 0.15625, Mean Loss 2.8461106823336695, Batch 7680 of 45000\n",
      "Epoch 0, Loss 2.652815818786621, Acc 0.125, Mean Loss 2.840070217847824, Batch 7936 of 45000\n",
      "Epoch 0, Loss 2.6452670097351074, Acc 0.1875, Mean Loss 2.834167090329257, Batch 8192 of 45000\n",
      "Epoch 0, Loss 2.7523436546325684, Acc 0.1875, Mean Loss 2.831760518691119, Batch 8448 of 45000\n",
      "Epoch 0, Loss 2.6834518909454346, Acc 0.1875, Mean Loss 2.8275231293269565, Batch 8704 of 45000\n",
      "Epoch 0, Loss 2.932039260864258, Acc 0.1875, Mean Loss 2.830426355202993, Batch 8960 of 45000\n",
      "Epoch 0, Loss 2.620882511138916, Acc 0.1875, Mean Loss 2.824763008066126, Batch 9216 of 45000\n",
      "Epoch 0, Loss 2.5868752002716064, Acc 0.28125, Mean Loss 2.818502802597849, Batch 9472 of 45000\n",
      "Epoch 0, Loss 2.621788263320923, Acc 0.3125, Mean Loss 2.8134588400522866, Batch 9728 of 45000\n",
      "Epoch 0, Loss 2.637972354888916, Acc 0.1875, Mean Loss 2.8090716779232023, Batch 9984 of 45000\n",
      "Epoch 0, Loss 2.4563190937042236, Acc 0.28125, Mean Loss 2.800467956356886, Batch 10240 of 45000\n",
      "Epoch 0, Loss 2.9327821731567383, Acc 0.1875, Mean Loss 2.8036182948521207, Batch 10496 of 45000\n",
      "Epoch 0, Loss 2.8149008750915527, Acc 0.09375, Mean Loss 2.803880680439084, Batch 10752 of 45000\n",
      "Epoch 0, Loss 2.6292479038238525, Acc 0.3125, Mean Loss 2.799911753697829, Batch 11008 of 45000\n",
      "Epoch 0, Loss 2.597597360610962, Acc 0.34375, Mean Loss 2.7954158782958984, Batch 11264 of 45000\n",
      "Epoch 0, Loss 2.614821672439575, Acc 0.25, Mean Loss 2.791489917299022, Batch 11520 of 45000\n",
      "Epoch 0, Loss 2.4105074405670166, Acc 0.40625, Mean Loss 2.7833839071557875, Batch 11776 of 45000\n",
      "Epoch 0, Loss 2.6352853775024414, Acc 0.3125, Mean Loss 2.780298521121343, Batch 12032 of 45000\n",
      "Epoch 0, Loss 2.8652472496032715, Acc 0.125, Mean Loss 2.7820321686413823, Batch 12288 of 45000\n",
      "Epoch 0, Loss 2.63744854927063, Acc 0.3125, Mean Loss 2.779140496253967, Batch 12544 of 45000\n",
      "Epoch 0, Loss 2.5059776306152344, Acc 0.28125, Mean Loss 2.7737843616336, Batch 12800 of 45000\n",
      "Epoch 0, Loss 2.8867647647857666, Acc 0.15625, Mean Loss 2.7759570616942186, Batch 13056 of 45000\n",
      "Epoch 0, Loss 2.8079981803894043, Acc 0.15625, Mean Loss 2.7765616111035616, Batch 13312 of 45000\n",
      "Epoch 0, Loss 2.7323801517486572, Acc 0.125, Mean Loss 2.775743435930323, Batch 13568 of 45000\n",
      "Epoch 0, Loss 2.790377140045166, Acc 0.25, Mean Loss 2.7760095032778653, Batch 13824 of 45000\n",
      "Epoch 0, Loss 2.901423931121826, Acc 0.09375, Mean Loss 2.778249046632222, Batch 14080 of 45000\n",
      "Epoch 0, Loss 2.721339702606201, Acc 0.21875, Mean Loss 2.7772506370879055, Batch 14336 of 45000\n",
      "Epoch 0, Loss 2.797781467437744, Acc 0.1875, Mean Loss 2.7776046169215234, Batch 14592 of 45000\n",
      "Epoch 0, Loss 2.7452211380004883, Acc 0.25, Mean Loss 2.777055744397438, Batch 14848 of 45000\n",
      "Epoch 0, Loss 2.6698453426361084, Acc 0.1875, Mean Loss 2.7752689043680827, Batch 15104 of 45000\n",
      "Epoch 0, Loss 2.557906150817871, Acc 0.375, Mean Loss 2.7717055805393906, Batch 15360 of 45000\n",
      "Epoch 0, Loss 2.514986038208008, Acc 0.21875, Mean Loss 2.767564942759852, Batch 15616 of 45000\n",
      "Epoch 0, Loss 2.752047061920166, Acc 0.1875, Mean Loss 2.7673186271909684, Batch 15872 of 45000\n",
      "Epoch 0, Loss 2.5017991065979004, Acc 0.21875, Mean Loss 2.7631698846817017, Batch 16128 of 45000\n",
      "Epoch 0, Loss 2.8787569999694824, Acc 0.125, Mean Loss 2.764948147993821, Batch 16384 of 45000\n",
      "Epoch 0, Loss 2.4165236949920654, Acc 0.4375, Mean Loss 2.759668989615007, Batch 16640 of 45000\n",
      "Epoch 0, Loss 2.769300937652588, Acc 0.125, Mean Loss 2.759812750033478, Batch 16896 of 45000\n",
      "Epoch 0, Loss 2.6583547592163086, Acc 0.1875, Mean Loss 2.758320720756755, Batch 17152 of 45000\n",
      "Epoch 0, Loss 2.695767641067505, Acc 0.15625, Mean Loss 2.7574141543844473, Batch 17408 of 45000\n",
      "Epoch 0, Loss 2.9133713245391846, Acc 0.125, Mean Loss 2.7596421139580865, Batch 17664 of 45000\n",
      "Epoch 0, Loss 2.518834114074707, Acc 0.3125, Mean Loss 2.756250451987898, Batch 17920 of 45000\n",
      "Epoch 0, Loss 2.824458599090576, Acc 0.25, Mean Loss 2.757197787364324, Batch 18176 of 45000\n",
      "Epoch 0, Loss 2.7459349632263184, Acc 0.15625, Mean Loss 2.7570435021021593, Batch 18432 of 45000\n",
      "Epoch 0, Loss 2.6057498455047607, Acc 0.21875, Mean Loss 2.7549989932292216, Batch 18688 of 45000\n",
      "Epoch 0, Loss 2.7386631965637207, Acc 0.1875, Mean Loss 2.7547811826070148, Batch 18944 of 45000\n",
      "Epoch 0, Loss 2.680793046951294, Acc 0.15625, Mean Loss 2.7538076545062817, Batch 19200 of 45000\n",
      "Epoch 0, Loss 2.874931573867798, Acc 0.15625, Mean Loss 2.7553806924200677, Batch 19456 of 45000\n",
      "Epoch 0, Loss 2.8117239475250244, Acc 0.125, Mean Loss 2.75610304184449, Batch 19712 of 45000\n",
      "Epoch 0, Loss 2.519282341003418, Acc 0.28125, Mean Loss 2.753105311454097, Batch 19968 of 45000\n",
      "Epoch 0, Loss 2.627455711364746, Acc 0.21875, Mean Loss 2.75153469145298, Batch 20224 of 45000\n",
      "Epoch 0, Loss 2.839099407196045, Acc 0.1875, Mean Loss 2.752615737326351, Batch 20480 of 45000\n",
      "Epoch 0, Loss 2.674058198928833, Acc 0.34375, Mean Loss 2.751657718565406, Batch 20736 of 45000\n",
      "Epoch 0, Loss 2.834117889404297, Acc 0.09375, Mean Loss 2.7526512145996094, Batch 20992 of 45000\n",
      "Epoch 0, Loss 2.858414649963379, Acc 0.25, Mean Loss 2.7539103031158447, Batch 21248 of 45000\n",
      "Epoch 0, Loss 2.553706645965576, Acc 0.28125, Mean Loss 2.7515549659729004, Batch 21504 of 45000\n",
      "Epoch 0, Loss 2.818859100341797, Acc 0.15625, Mean Loss 2.752337572186492, Batch 21760 of 45000\n",
      "Epoch 0, Loss 2.775578737258911, Acc 0.1875, Mean Loss 2.752604712014911, Batch 22016 of 45000\n",
      "Epoch 0, Loss 2.65167498588562, Acc 0.375, Mean Loss 2.7514577833088962, Batch 22272 of 45000\n",
      "Epoch 0, Loss 2.844399929046631, Acc 0.1875, Mean Loss 2.75250207708123, Batch 22528 of 45000\n",
      "Epoch 0, Loss 2.535973310470581, Acc 0.375, Mean Loss 2.7500962018966675, Batch 22784 of 45000\n",
      "Epoch 0, Loss 2.5357096195220947, Acc 0.3125, Mean Loss 2.747740305387057, Batch 23040 of 45000\n",
      "Epoch 0, Loss 2.6921048164367676, Acc 0.21875, Mean Loss 2.7471355718115102, Batch 23296 of 45000\n",
      "Epoch 0, Loss 2.373809814453125, Acc 0.3125, Mean Loss 2.7431213163560435, Batch 23552 of 45000\n",
      "Epoch 0, Loss 2.6380271911621094, Acc 0.25, Mean Loss 2.7420032937475978, Batch 23808 of 45000\n",
      "Epoch 0, Loss 2.6206634044647217, Acc 0.15625, Mean Loss 2.740726031755146, Batch 24064 of 45000\n",
      "Epoch 0, Loss 2.4614763259887695, Acc 0.34375, Mean Loss 2.7378171806534133, Batch 24320 of 45000\n",
      "Epoch 0, Loss 2.476841688156128, Acc 0.28125, Mean Loss 2.7351267116585958, Batch 24576 of 45000\n",
      "Epoch 0, Loss 2.867363691329956, Acc 0.125, Mean Loss 2.7364760685940177, Batch 24832 of 45000\n",
      "Epoch 0, Loss 2.49377703666687, Acc 0.3125, Mean Loss 2.7340245632210163, Batch 25088 of 45000\n",
      "Epoch 0, Loss 2.677987575531006, Acc 0.15625, Mean Loss 2.7334641933441164, Batch 25344 of 45000\n",
      "Epoch 0, Loss 2.680651903152466, Acc 0.21875, Mean Loss 2.7329412993818227, Batch 25600 of 45000\n",
      "Epoch 0, Loss 2.4832956790924072, Acc 0.25, Mean Loss 2.7304937933005538, Batch 25856 of 45000\n",
      "Epoch 0, Loss 2.730907917022705, Acc 0.15625, Mean Loss 2.7304978139192153, Batch 26112 of 45000\n",
      "Epoch 0, Loss 2.49660325050354, Acc 0.28125, Mean Loss 2.7282488277325263, Batch 26368 of 45000\n",
      "Epoch 0, Loss 2.6458659172058105, Acc 0.3125, Mean Loss 2.727464228584653, Batch 26624 of 45000\n",
      "Epoch 0, Loss 2.71835994720459, Acc 0.28125, Mean Loss 2.727378339137671, Batch 26880 of 45000\n",
      "Epoch 0, Loss 2.532519578933716, Acc 0.21875, Mean Loss 2.725557229229223, Batch 27136 of 45000\n",
      "Epoch 0, Loss 2.6960020065307617, Acc 0.25, Mean Loss 2.725283569759793, Batch 27392 of 45000\n",
      "Epoch 0, Loss 2.6084532737731934, Acc 0.25, Mean Loss 2.724211732181934, Batch 27648 of 45000\n",
      "Epoch 0, Loss 2.6449966430664062, Acc 0.25, Mean Loss 2.7234915950081566, Batch 27904 of 45000\n",
      "Epoch 0, Loss 2.2548041343688965, Acc 0.4375, Mean Loss 2.719269185452848, Batch 28160 of 45000\n",
      "Epoch 0, Loss 2.5372538566589355, Acc 0.3125, Mean Loss 2.7176440485886166, Batch 28416 of 45000\n",
      "Epoch 0, Loss 2.790923833847046, Acc 0.21875, Mean Loss 2.71829254226347, Batch 28672 of 45000\n",
      "Epoch 0, Loss 2.6315560340881348, Acc 0.3125, Mean Loss 2.7175316957005284, Batch 28928 of 45000\n",
      "Epoch 0, Loss 2.6776387691497803, Acc 0.21875, Mean Loss 2.7171848006870434, Batch 29184 of 45000\n",
      "Epoch 0, Loss 2.682685375213623, Acc 0.1875, Mean Loss 2.7168873918467553, Batch 29440 of 45000\n",
      "Epoch 0, Loss 2.582594871520996, Acc 0.3125, Mean Loss 2.715739592527732, Batch 29696 of 45000\n",
      "Epoch 0, Loss 2.8283770084381104, Acc 0.09375, Mean Loss 2.7166941468998536, Batch 29952 of 45000\n",
      "Epoch 0, Loss 2.577737808227539, Acc 0.21875, Mean Loss 2.7155264465748763, Batch 30208 of 45000\n",
      "Epoch 0, Loss 2.4593238830566406, Acc 0.3125, Mean Loss 2.713391425212224, Batch 30464 of 45000\n",
      "Epoch 0, Loss 2.8348042964935303, Acc 0.15625, Mean Loss 2.714394837371574, Batch 30720 of 45000\n",
      "Epoch 0, Loss 2.47283673286438, Acc 0.28125, Mean Loss 2.7124148529084002, Batch 30976 of 45000\n",
      "Epoch 0, Loss 2.470621347427368, Acc 0.28125, Mean Loss 2.710449052050831, Batch 31232 of 45000\n",
      "Epoch 0, Loss 2.5676231384277344, Acc 0.28125, Mean Loss 2.7092972301667735, Batch 31488 of 45000\n",
      "Epoch 0, Loss 2.5685160160064697, Acc 0.28125, Mean Loss 2.7081709804534913, Batch 31744 of 45000\n",
      "Epoch 0, Loss 2.708345890045166, Acc 0.1875, Mean Loss 2.7081723686248536, Batch 32000 of 45000\n",
      "Epoch 0, Loss 2.56890606880188, Acc 0.25, Mean Loss 2.7070757835868777, Batch 32256 of 45000\n",
      "Epoch 0, Loss 2.4279000759124756, Acc 0.3125, Mean Loss 2.7048947233706713, Batch 32512 of 45000\n",
      "Epoch 0, Loss 2.4125678539276123, Acc 0.34375, Mean Loss 2.7026286236075467, Batch 32768 of 45000\n",
      "Epoch 0, Loss 2.503572463989258, Acc 0.28125, Mean Loss 2.7010974223797137, Batch 33024 of 45000\n",
      "Epoch 0, Loss 2.397695541381836, Acc 0.375, Mean Loss 2.6987813774866, Batch 33280 of 45000\n",
      "Epoch 0, Loss 2.524855136871338, Acc 0.25, Mean Loss 2.697463754451636, Batch 33536 of 45000\n",
      "Epoch 0, Loss 2.314939022064209, Acc 0.28125, Mean Loss 2.6945876286442116, Batch 33792 of 45000\n",
      "Epoch 0, Loss 2.740342617034912, Acc 0.21875, Mean Loss 2.6949290837814557, Batch 34048 of 45000\n",
      "Epoch 0, Loss 2.6206493377685547, Acc 0.25, Mean Loss 2.6943788634406194, Batch 34304 of 45000\n",
      "Epoch 0, Loss 2.571833848953247, Acc 0.1875, Mean Loss 2.693477797157624, Batch 34560 of 45000\n",
      "Epoch 0, Loss 2.7416512966156006, Acc 0.15625, Mean Loss 2.6938294285405293, Batch 34816 of 45000\n",
      "Epoch 0, Loss 2.6380038261413574, Acc 0.25, Mean Loss 2.6934248951898105, Batch 35072 of 45000\n",
      "Epoch 0, Loss 2.3829987049102783, Acc 0.28125, Mean Loss 2.6911916132453535, Batch 35328 of 45000\n",
      "Epoch 0, Loss 2.6717193126678467, Acc 0.1875, Mean Loss 2.6910525253840856, Batch 35584 of 45000\n",
      "Epoch 0, Loss 2.602297067642212, Acc 0.1875, Mean Loss 2.690423054052583, Batch 35840 of 45000\n",
      "Epoch 0, Loss 2.2571394443511963, Acc 0.40625, Mean Loss 2.6873717610265166, Batch 36096 of 45000\n",
      "Epoch 0, Loss 2.568930149078369, Acc 0.21875, Mean Loss 2.6865434980059004, Batch 36352 of 45000\n",
      "Epoch 0, Loss 2.6729509830474854, Acc 0.25, Mean Loss 2.686449105540911, Batch 36608 of 45000\n",
      "Epoch 0, Loss 2.6269736289978027, Acc 0.28125, Mean Loss 2.686038929840614, Batch 36864 of 45000\n",
      "Epoch 0, Loss 2.5197622776031494, Acc 0.25, Mean Loss 2.6849000486609054, Batch 37120 of 45000\n",
      "Epoch 0, Loss 2.4483587741851807, Acc 0.28125, Mean Loss 2.683290924344744, Batch 37376 of 45000\n",
      "Epoch 0, Loss 2.360766649246216, Acc 0.40625, Mean Loss 2.681111706269754, Batch 37632 of 45000\n",
      "Epoch 0, Loss 2.4273698329925537, Acc 0.28125, Mean Loss 2.6794087406772897, Batch 37888 of 45000\n",
      "Epoch 0, Loss 2.689551830291748, Acc 0.25, Mean Loss 2.679476361274719, Batch 38144 of 45000\n",
      "Epoch 0, Loss 2.7434611320495605, Acc 0.125, Mean Loss 2.679900101478526, Batch 38400 of 45000\n",
      "Epoch 0, Loss 2.242719888687134, Acc 0.34375, Mean Loss 2.6770239158680567, Batch 38656 of 45000\n",
      "Epoch 0, Loss 2.705990791320801, Acc 0.15625, Mean Loss 2.677213241851408, Batch 38912 of 45000\n",
      "Epoch 0, Loss 2.372995138168335, Acc 0.25, Mean Loss 2.6752377996196994, Batch 39168 of 45000\n",
      "Epoch 0, Loss 2.3971595764160156, Acc 0.28125, Mean Loss 2.6734437465667726, Batch 39424 of 45000\n",
      "Epoch 0, Loss 2.717517614364624, Acc 0.15625, Mean Loss 2.6737262713603482, Batch 39680 of 45000\n",
      "Epoch 0, Loss 2.697688102722168, Acc 0.21875, Mean Loss 2.6738788944900413, Batch 39936 of 45000\n",
      "Epoch 0, Loss 2.5827689170837402, Acc 0.3125, Mean Loss 2.6733022490634193, Batch 40192 of 45000\n",
      "Epoch 0, Loss 2.5987727642059326, Acc 0.34375, Mean Loss 2.6728335101649447, Batch 40448 of 45000\n",
      "Epoch 0, Loss 2.4727516174316406, Acc 0.3125, Mean Loss 2.6715829983353614, Batch 40704 of 45000\n",
      "Epoch 0, Loss 2.5630042552948, Acc 0.25, Mean Loss 2.670908596204675, Batch 40960 of 45000\n",
      "Epoch 0, Loss 2.285606622695923, Acc 0.5, Mean Loss 2.6685301889607937, Batch 41216 of 45000\n",
      "Epoch 0, Loss 2.4264066219329834, Acc 0.25, Mean Loss 2.6670447683041814, Batch 41472 of 45000\n",
      "Epoch 0, Loss 2.3576407432556152, Acc 0.4375, Mean Loss 2.6651581583953483, Batch 41728 of 45000\n",
      "Epoch 0, Loss 2.760164260864258, Acc 0.1875, Mean Loss 2.665733952955766, Batch 41984 of 45000\n",
      "Epoch 0, Loss 2.4676733016967773, Acc 0.28125, Mean Loss 2.6645408165023987, Batch 42240 of 45000\n",
      "Epoch 0, Loss 2.4711928367614746, Acc 0.28125, Mean Loss 2.663383044168621, Batch 42496 of 45000\n",
      "Epoch 0, Loss 2.5780346393585205, Acc 0.28125, Mean Loss 2.662875017949513, Batch 42752 of 45000\n",
      "Epoch 0, Loss 2.3000245094299316, Acc 0.46875, Mean Loss 2.660727973520403, Batch 43008 of 45000\n",
      "Epoch 0, Loss 2.6660397052764893, Acc 0.1875, Mean Loss 2.660759219001321, Batch 43264 of 45000\n",
      "Epoch 0, Loss 2.7107625007629395, Acc 0.15625, Mean Loss 2.6610516358537284, Batch 43520 of 45000\n",
      "Epoch 0, Loss 2.2634918689727783, Acc 0.40625, Mean Loss 2.6587402418602344, Batch 43776 of 45000\n",
      "Epoch 0, Loss 2.420626640319824, Acc 0.3125, Mean Loss 2.657363862660579, Batch 44032 of 45000\n",
      "Epoch 0, Loss 2.4648571014404297, Acc 0.1875, Mean Loss 2.6562575019639114, Batch 44288 of 45000\n",
      "Epoch 0, Loss 2.384702682495117, Acc 0.34375, Mean Loss 2.6547057601383752, Batch 44544 of 45000\n",
      "Epoch 0, Loss 2.678591728210449, Acc 0.1875, Mean Loss 2.6548414758660575, Batch 44800 of 45000\n",
      "Epoch 1, Loss 2.4235262870788574, Acc 0.25, Mean Loss 2.6535346103926836, Batch 0 of 45000\n",
      "Epoch 1, Loss 2.526182174682617, Acc 0.28125, Mean Loss 2.652819147270717, Batch 256 of 45000\n",
      "Epoch 1, Loss 2.5056188106536865, Acc 0.21875, Mean Loss 2.6519967990214597, Batch 512 of 45000\n",
      "Epoch 1, Loss 2.293898582458496, Acc 0.40625, Mean Loss 2.650007364484999, Batch 768 of 45000\n",
      "Epoch 1, Loss 2.673384189605713, Acc 0.21875, Mean Loss 2.6501365182149477, Batch 1024 of 45000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m output \u001b[38;5;241m=\u001b[39m classifier(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, ys)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     46\u001b[0m acc \u001b[38;5;241m=\u001b[39m (output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m ys)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the DistilBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Instantiate the model\n",
    "classifier = DistilBERTClassifier(model, n_classes)\n",
    "\n",
    "# Move the model to the device\n",
    "classifier.to(device)\n",
    "\n",
    "# Dataset\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "\n",
    "# Convert train, val, and test\n",
    "xs_train = dataset['train']['text']\n",
    "ys_train = dataset['train']['label']\n",
    "\n",
    "xs_tok_train = tokenizer(xs_train, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "losses = []\n",
    "accuracy = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=5e-5)\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "len_train = len(xs_train)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len_train, batch_size):\n",
    "        batch = xs_tok_train['input_ids'][i:i+32], xs_tok_train['attention_mask'][i:i+32]\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        ys = torch.tensor(ys_train[i:i+32]).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(*batch)\n",
    "        loss = criterion(output, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = (output.argmax(1) == ys).float().mean()\n",
    "        losses.append(loss.item())\n",
    "        accuracy.append(acc.item())\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item()}, Acc {acc.item()}, Mean Loss {sum(losses)/len(losses)}, Batch {i} of {len_train}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
