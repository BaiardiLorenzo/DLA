{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "809f8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd466d3b-cc41-4de3-9f82-3547569909f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "class Dante:\n",
    "    \"\"\"A class that aggregates functionality related to the \"corpus\" used.\"\"\"\n",
    "    def __init__(self, train = True, train_size=0.9, block_size=128):\n",
    "        self._block_size = block_size\n",
    "        self._train = train\n",
    "\n",
    "        #Load entier text file\n",
    "        with open('commedia.txt', 'r', encoding='utf-8') as fd:\n",
    "            rawdata = fd.read()\n",
    "\n",
    "        # Extract tokend BEFORE splitting. Our tokens are characters.\n",
    "        self._tokens = sorted(set(rawdata))\n",
    "        self.num_tokens = len(self._tokens)\n",
    "\n",
    "        # Select train or val/test set.\n",
    "        rawdata = rawdata[:int(len(rawdata)*train_size)] if train else rawdata[int(len(rawdata)*train_size):]\n",
    "\n",
    "        # Build the encode/decode dictionaries mapping chars to token ids and back.\n",
    "        self._c2i = {c: i for (i, c) in enumerate(self._tokens)}\n",
    "        self._i2c = {i: c for (i, c) in enumerate(self._tokens)}\n",
    "\n",
    "        # Encode \n",
    "        self.encode = lambda s: [self._c2i[c] for c in s] # encoder: take a string, output a list of integers\n",
    "        self.decode = lambda l: ''.join([self._i2c[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "        # Encode the data\n",
    "        self._data = torch.tensor(self.encode(rawdata), dtype=torch.long)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        \"\"\" Retrives a random batch of context and targets.\"\"\"\n",
    "        ix = torch.randint(len(self._data) - self._block_size, (batch_size,))\n",
    "        print(self._data)\n",
    "        x = torch.stack([self._data[i:i+self._block_size] for i in ix])\n",
    "        y = torch.stack([self._data[i+1:i+self._block_size+1] for i in ix])\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data) - self._block_size - 1\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        xs = self._data[i:i+self._block_size]\n",
    "        ys = self._data[i+1:i+self._block_size+1]\n",
    "        return (xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f6e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dante(train=True, train_size=0.9, block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd83c799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75, 66, 73, 1, 74, 66, 87, 87, 76, 1, 65, 66, 73, 1, 64, 62, 74, 74, 70, 75]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode a string\n",
    "ds.encode('nel mezzo del cammin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f95abb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nel mezzo del cammin'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode an Encoded string -> return 'nel mezzo del cammin'\n",
    "ds.decode(ds.encode('nel mezzo del cammin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f637104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([51, 69, 66,  ..., 81, 79, 76])\n"
     ]
    }
   ],
   "source": [
    "(xs, ys) = ds.get_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac55f640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128]), torch.Size([32, 128]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaa13d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([70, 62, 11,  1, 73, 66, 81, 81, 76, 79, 11,  1, 81, 70,  1, 68, 70, 82,\n",
       "        79, 76, 11,  0,  1,  1, 80,  7, 66, 73, 73, 66,  1, 75, 76, 75,  1, 80,\n",
       "        70, 66, 75,  1, 65, 70,  1, 73, 82, 75, 68, 62,  1, 68, 79, 62, 87, 70,\n",
       "        62,  1, 83, 76, 81, 66, 11,  0,  0, 64, 69,  7, 70,  7,  1, 83, 70, 65,\n",
       "        70,  1, 77, 66, 79,  1, 78, 82, 66, 73, 73,  7, 62, 66, 79, 66,  1, 68,\n",
       "        79, 76, 80, 80, 76,  1, 66,  1, 80, 64, 82, 79, 76,  0,  1,  1, 83, 66,\n",
       "        75, 70, 79,  1, 75, 76, 81, 62, 75, 65, 76,  1, 82, 75, 62,  1, 67, 70,\n",
       "        68, 82])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"First input\"\n",
    "xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d62f333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ia, lettor, ti giuro,\\n  s'elle non sien di lunga grazia vote,\\n\\nch'i' vidi per quell'aere grosso e scuro\\n  venir notando una figu\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds.decode(xs[0]) Not working\n",
    "ds.decode(xs[0].numpy()) # Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc21504b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a, lettor, ti giuro,\\n  s'elle non sien di lunga grazia vote,\\n\\nch'i' vidi per quell'aere grosso e scuro\\n  venir notando una figur\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.decode(ys[0].numpy()) # Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "986c204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All configuration parameters for out Transformer\n",
    "block_size = 128\n",
    "train_size = 0.9\n",
    "batch_size = 32\n",
    "n_embed = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af5d1abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11,  0,  1,  ..., 59,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate datasets for training and test\n",
    "ds_train = Dante(train=True, train_size=train_size, block_size=block_size)\n",
    "ds_test = Dante(train=False, train_size=train_size, block_size=block_size)\n",
    "(xs, ys) = ds_test.get_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dedf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top-level GPT nn.Module\n",
    "class GTPLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed):\n",
    "        super().__init__()\n",
    "        self._vocab_size = vocab_size\n",
    "        self._n_embd = n_embed\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        (B, T) = idx.shape\n",
    "        tok_emb = self.token_embedding(idx) # (B, T, C)\n",
    "        return tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce065322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTPLanguageModel(vocab_size=ds_train.num_tokens, n_embed = n_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c798bf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0182,  0.6876,  0.9524, -0.1810,  0.9837, -0.6089, -1.1648,  0.2752,\n",
       "        -0.6810,  0.4400,  1.3942, -0.2972,  0.2556,  1.7411, -0.1625,  0.7471,\n",
       "        -0.3994,  0.6829,  0.6663, -1.9936, -1.0045,  0.6590,  1.0105, -0.0707,\n",
       "         1.5947,  0.0098,  0.7688, -0.8266, -0.4158,  1.1425, -0.6613,  0.3734,\n",
       "        -0.3173, -0.1288,  1.8279, -0.1044,  1.3437,  1.6375,  1.3891,  0.1766,\n",
       "        -1.1703,  0.6529,  0.9052,  0.4542,  0.8510,  0.0475, -1.1846,  0.7598,\n",
       "         1.0428,  1.2485, -0.1313, -0.1652,  0.0153, -0.1453,  0.8056,  0.1221,\n",
       "        -1.8702, -0.1466,  0.7614,  0.3381, -0.6846, -1.0877,  1.8149, -0.5938,\n",
       "        -0.3843,  1.2736,  1.1190, -0.9846,  0.2179, -0.1396,  0.3629,  0.3197,\n",
       "         0.8835, -0.4273, -0.9002,  0.1076, -1.4472,  0.2919, -1.0444, -0.3461,\n",
       "         0.9479,  0.7831, -1.8522,  1.7290,  2.5879,  0.3881,  0.2460,  0.6543,\n",
       "         0.6894,  1.1303,  0.3790, -0.6986, -1.5515,  0.2599, -0.7662, -2.3683,\n",
       "         0.2489, -0.9762,  0.9289,  0.8374,  0.1356,  2.2834,  0.4203, -0.5832,\n",
       "        -1.1883, -1.4646,  0.1313, -0.1989,  0.3273, -0.8213, -1.7832, -1.8499,\n",
       "        -0.9608,  0.5331,  1.4874,  1.5569,  1.0515, -1.4750, -0.4488, -1.7029,\n",
       "        -1.2705, -0.9757,  0.0098,  1.2787, -0.0545, -1.0491, -1.0499,  0.1849],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xs)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loreb\\miniconda3\\envs\\DLA\\Lib\\site-packages\\huggingface_hub-0.23.0-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   45,   417,   502, 47802,  1619, 12172,  1084,  2566, 18216,   430,\n",
      "           410,  5350]])\n",
      "tensor([[   34, 13481, 21504,   442,  1789,    78, 34898]])\n",
      "tensor([[28875, 14057, 14266,   952]])\n",
      "tensor([[   35, 12427,   435,   394, 29864]])\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "# Load key classes GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "print(tokenizer(\"Nel mezzo del cammin di nostra vita\", return_tensors='pt')[\"input_ids\"])\n",
    "print(tokenizer(\"Ciao mi chiamo Dante\", return_tensors='pt')[\"input_ids\"])\n",
    "print(tokenizer(\"Paolo Brosio\", return_tensors='pt')[\"input_ids\"])\n",
    "print(tokenizer(\"Dante alighieri\", return_tensors='pt')[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nel mezzo del cammin di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra vita, di nostra\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "# Load key classes GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Generate text from a prompt\n",
    "prompt = \"Nel mezzo del cammin di nostra vita\"\n",
    "generated = model.generate(tokenizer(prompt, return_tensors='pt')[\"input_ids\"], max_length=100)\n",
    "# print(generated)\n",
    "print(tokenizer.decode(generated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58081878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nel mezzo del cammin di nostra vita faggio alla e di siguella, una di giorale della santo e una perche di che di sanna.\n",
      "\n",
      "D'autre pela susere quelequando sugliando il sable l'apicher che l'ampli sopentor pitta di spagna, neque l'argento e sebata e che qu\n"
     ]
    }
   ],
   "source": [
    "# Look at the do_sample and temperature parameters\n",
    "generated = model.generate(tokenizer(prompt, return_tensors='pt')[\"input_ids\"], max_length=100, do_sample=True, temperature=0.9)   \n",
    "# print(generated)\n",
    "print(tokenizer.decode(generated[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
