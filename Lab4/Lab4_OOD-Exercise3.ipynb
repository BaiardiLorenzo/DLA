{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Exercise 3: Wildcard\n",
    "\n",
    "You know the drill. Pick *ONE* of the following exercises to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Exercise 3.1: Implement ODIN for OOD detection\n",
    "ODIN is a very simple approach, and you can already start experimenting by implementing a temperature hyperparameter in your base model and doing a grid search on $T$ and $\\varepsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import FakeData\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "fakeset = FakeData(size=1000, image_size=(3, 32, 32), transform=transform)\n",
    "fakeloader = torch.utils.data.DataLoader(fakeset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple CNN model.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS CELL TO LOAD THE PRETRAINED MODEL.\n",
    "device = 'cpu'\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load('./OOD_CNN.pth',map_location=torch.device('cpu'))) #50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_softmax(logit, T=1.0, delta= 0.5):\n",
    "    s = F.softmax(logit/T, 1)\n",
    "    s = s.max(dim=1)[0] #get the max for each element of the batch\n",
    "    return s\n",
    "    \n",
    "\n",
    "\n",
    "def compute_scores(data_loader, score_fun):\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            x, y = data\n",
    "            output = model(x.to(device))\n",
    "            s = score_fun(output)\n",
    "            scores.append(s)\n",
    "        scores_t = torch.cat(scores)\n",
    "        return scores_t\n",
    "\n",
    "\n",
    "def OOD_det(model, data_loader, T, eps, delta):\n",
    "    score = []\n",
    "    for data in data_loader:\n",
    "        model.train()\n",
    "        x,y = data\n",
    "        \n",
    "        x.requires_grad=True \n",
    "\n",
    "        x.retain_grad()\n",
    "\n",
    "        output=model(x)\n",
    "        model.zero_grad()\n",
    "        l = loss(output,y)\n",
    "        l.backward()\n",
    "        v = x + eps*torch.sign(x.grad)#fgsm\n",
    "        with torch.no_grad():\n",
    "            logit = model(v)\n",
    "            s = max_softmax(logit, T, delta)\n",
    "            for i in range(np.shape(s)[0]): #calcolo della funzione g\n",
    "                if s[i]>delta:\n",
    "                    score.append(1)\n",
    "                else:\n",
    "                    score.append(0)\n",
    "    score_t = torch.tensor(score)\n",
    "    return score_t\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GRID SEARCH ON EPS, T, DELTA(?). Uso test set come validation set.\n",
    "\n",
    "\n",
    "e = np.linspace(0, 0.004, 10)#10 valori da 0 a 0.004 come nel paper su ODIN\n",
    "t = np.array([1, 5, 10, 50, 100, 500, 1000])\n",
    "\n",
    "for eps in e:\n",
    "    for temp in t:\n",
    "        scores_fake= OOD_det(model, fakeloader, temp, eps, delta=0.5)\n",
    "        scores_test= OOD_det(model, testloader, temp, eps, delta=0.5)\n",
    "        y_test = torch.ones_like(scores_test)\n",
    "        y_fake = torch.zeros_like(scores_fake)\n",
    "        y = torch.cat((y_test, y_fake))\n",
    "        y_pred = torch.cat((scores_test, scores_fake))\n",
    "        s = metrics.roc_auc_score(y, y_pred)\n",
    "        print(\"AUC for eps = %s, temp = %s is %s\" % (eps, temp, s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Exercise 3.2: Implement JARN\n",
    "In exercise 2.2 you already implemented Jacobian-regularized learning to make your model more robust to adversarial samples. Add a *discriminator* to your model to encourage the adversarial samples used for regularization to be more *salient*.\n",
    "\n",
    "See [the JARN paper](https://arxiv.org/abs/1912.10185) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import FakeData\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=8)\n",
    "\n",
    "fakeset = FakeData(size=1000, image_size=(3, 32, 32), transform=transform)\n",
    "fakeloader = torch.utils.data.DataLoader(fakeset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=8)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple CNN model.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Adaptor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3,3,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.tanh(x)\n",
    "        return x\n",
    "     \n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_net = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(3, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.MaxPool2d(2,2)\n",
    "            )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(64*10*10, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x= torch.flatten(x,1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS CELL TO LOAD THE PRETRAINED MODEL.\n",
    "device = 'cpu'\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model = CNN().to(device)\n",
    "apt = Adaptor().to(device)\n",
    "disc = Discriminator().to(device)\n",
    "#model.load_state_dict(torch.load('./OOD_CNN.pth',map_location=torch.device('cpu'))) #50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model = CNN().to(device)\n",
    "apt = Adaptor().to(device)\n",
    "disc = Discriminator().to(device)\n",
    "\n",
    "for data in testloader:\n",
    "    x, y = data\n",
    "    break\n",
    "\n",
    "x1,y1 = x,y\n",
    "a = 0.0001\n",
    "u = 1\n",
    "model1 = model\n",
    "apt1 = apt\n",
    "disc_new =disc\n",
    "opt_model1 = torch.optim.Adam(model1.parameters(), lr=a)\n",
    "opt_apt = torch.optim.Adam(apt.parameters(), lr=a)\n",
    "opt_disc_new = torch.optim.Adam(disc_new.parameters(), lr=a)\n",
    "model1.train()\n",
    "model.train\n",
    "apt.train()\n",
    "disc.train()\n",
    "\n",
    "\n",
    "x.requires_grad = True\n",
    "output = model(x)\n",
    "x.retain_grad()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model.zero_grad()\n",
    "l = loss(output,y)\n",
    "l.backward()\n",
    "jacobian = x.grad\n",
    "x.requires_grad = False\n",
    "print(np.shape(jacobian))\n",
    "generated = apt1(jacobian)\n",
    "#CALCOLO LOSS ADVERSARIAL SFRUTTANDO DEFINIZIONE DI BCE\n",
    "\n",
    "loss1 = nn.CrossEntropyLoss()\n",
    "output1 = model1(x1)\n",
    "l1 = loss1(output1,y1)\n",
    "real = torch.ones_like(disc(x1))\n",
    "fake = torch.zeros_like(disc(generated.detach()))\n",
    "bce = nn.BCELoss()\n",
    "\n",
    "lreal = bce(real, disc(x1))\n",
    "lfake = bce(fake, disc(generated))\n",
    "l_adv = lreal + lfake\n",
    "#CALCOLO LOSS DI TRAINING, NOMI LOSS ASSEGNATI SECONDO I PARAMETRI DEL PAPER\n",
    "l_theta = l1 + u*l_adv   #theta parametro di model \n",
    "\n",
    "opt_model1.zero_grad()\n",
    "l_theta.backward()\n",
    "opt_model1.step()\n",
    "\n",
    "\n",
    "generated1 = apt(jacobian.detach())\n",
    "lreal1 = bce(real,disc(x1.detach()))\n",
    "lfake1 = bce(fake, disc(generated1))\n",
    "l_adv1 = lreal1 + lfake1\n",
    "opt_apt.zero_grad()\n",
    "l_adv1.backward()\n",
    "opt_apt.step()\n",
    " \n",
    "\n",
    "\n",
    "lreal_disc = bce(real, disc_new(x1.detach()))\n",
    "lfake_disc = bce(fake, disc_new(generated1.detach()))\n",
    "l_adv_disc = lreal_disc + lfake_disc\n",
    "l_phi = -1*l_adv_disc\n",
    "opt_disc_new.zero_grad()\n",
    "l_adv_disc.backward()\n",
    "opt_disc_new.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_net.0.weight tensor([[[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]]])\n",
      "conv_net.0.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "conv_net.2.weight tensor([[[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]]])\n",
      "conv_net.2.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "conv_net.4.weight tensor([[[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan, nan]]]])\n",
      "conv_net.4.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "linear_layers.0.weight tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "linear_layers.0.bias tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0., nan, nan, -inf, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -inf, 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -inf, nan, nan, nan, 0., nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, inf, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0., nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0., nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, inf, nan, nan, nan, nan, nan, nan, -inf, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, inf, inf, nan, nan, nan, nan, nan, nan, 0., nan,\n",
      "        0., nan, nan, nan, nan, nan, nan, nan, nan, 0., 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -inf, nan, 0., nan, nan, 0., 0., nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, 0., nan, nan, nan, nan, nan, nan, nan, nan, 0., nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -inf, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, inf, nan, nan, nan, nan, nan, nan, nan, nan, inf, nan, nan, nan, nan, nan, -inf,\n",
      "        nan, nan, nan, nan, -inf, nan, nan, nan, nan, nan, inf, nan, nan, nan, nan, inf, 0., nan, nan, nan, nan, nan, nan, 0.,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "linear_layers.2.weight tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, inf, inf,  ..., nan, nan, nan]])\n",
      "linear_layers.2.bias tensor([0., nan, 0., 0., nan, 0., 0., nan, nan, inf])\n",
      "linear_layers.4.weight tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
      "linear_layers.4.bias tensor([nan])\n",
      "tensor([[[[ 0.0000e+00, -1.0723e-07, -3.3845e-08,  ..., -1.1500e-07,\n",
      "           -4.6081e-09, -5.6264e-09],\n",
      "          [ 0.0000e+00,  1.6107e-07,  2.0231e-07,  ..., -1.1875e-07,\n",
      "            3.9120e-09, -4.5208e-08],\n",
      "          [ 0.0000e+00, -4.9076e-07,  5.4680e-07,  ..., -1.8313e-07,\n",
      "           -2.1185e-07,  8.9431e-08],\n",
      "          ...,\n",
      "          [ 3.5517e-07,  3.7199e-07, -3.8230e-07,  ...,  2.3063e-08,\n",
      "            9.1601e-08,  3.7166e-09],\n",
      "          [ 4.9458e-08,  4.3995e-08, -1.3450e-07,  ..., -1.0078e-07,\n",
      "            1.0822e-08, -6.9448e-08],\n",
      "          [-1.4266e-07,  3.0269e-08, -5.1490e-08,  ..., -3.6492e-07,\n",
      "            2.0137e-08,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  2.6496e-09,  2.1908e-07,  ...,  4.5148e-08,\n",
      "            5.3188e-08, -2.3381e-08],\n",
      "          [ 0.0000e+00, -5.9055e-08, -2.3961e-07,  ..., -5.3736e-08,\n",
      "           -8.8124e-09, -4.6688e-09],\n",
      "          [ 0.0000e+00,  4.9294e-08,  1.6836e-07,  ..., -1.0452e-07,\n",
      "            3.1545e-07,  1.4770e-07],\n",
      "          ...,\n",
      "          [ 8.2027e-08,  2.9933e-07, -7.5162e-09,  ...,  5.5826e-07,\n",
      "            1.5698e-07,  9.0409e-09],\n",
      "          [ 1.7276e-08, -4.1156e-07,  1.5292e-07,  ..., -2.6062e-07,\n",
      "            1.2529e-07,  7.0529e-08],\n",
      "          [-1.0368e-07, -5.0704e-08,  6.3007e-08,  ...,  4.0772e-07,\n",
      "           -1.5909e-07,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00, -1.3017e-08, -9.7491e-08,  ...,  3.3428e-08,\n",
      "           -5.2685e-09, -3.7915e-08],\n",
      "          [ 0.0000e+00, -5.9336e-08,  4.0293e-08,  ..., -1.3383e-07,\n",
      "           -3.2272e-08, -5.0030e-08],\n",
      "          [ 0.0000e+00,  1.1560e-08, -5.7287e-07,  ...,  3.2567e-07,\n",
      "            1.1019e-07,  3.1508e-07],\n",
      "          ...,\n",
      "          [ 2.2125e-07, -5.2996e-07, -1.6234e-07,  ...,  3.8091e-07,\n",
      "           -6.3675e-08,  5.4857e-08],\n",
      "          [ 1.3227e-07, -4.5979e-09,  9.5505e-08,  ...,  1.4407e-08,\n",
      "           -1.8628e-07,  1.1567e-07],\n",
      "          [-1.1963e-07,  9.6437e-08,  1.4134e-07,  ...,  2.7168e-07,\n",
      "           -1.3450e-07,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.5285e-10, -4.7162e-09, -1.0873e-07,  ..., -4.7302e-07,\n",
      "           -6.1338e-08, -9.2746e-08],\n",
      "          [ 8.7645e-10,  3.5064e-09, -6.3383e-08,  ...,  3.9362e-07,\n",
      "           -2.3921e-07,  3.4734e-08],\n",
      "          [ 5.4706e-09,  3.6120e-09, -9.4240e-08,  ..., -2.9974e-07,\n",
      "            5.9023e-08, -6.5916e-09],\n",
      "          ...,\n",
      "          [-8.1894e-09,  3.3889e-08, -1.2234e-07,  ..., -2.8713e-07,\n",
      "            3.5294e-07,  3.6014e-07],\n",
      "          [ 3.8147e-08,  5.8534e-08, -6.8916e-08,  ...,  2.4158e-07,\n",
      "            1.6264e-07,  1.7320e-07],\n",
      "          [ 2.8746e-08, -1.7433e-08, -2.3041e-08,  ..., -2.1524e-07,\n",
      "            1.2719e-07,  4.0143e-08]],\n",
      "\n",
      "         [[-9.2476e-10, -3.9063e-09,  2.1691e-07,  ..., -9.4344e-08,\n",
      "           -7.9617e-08,  1.3935e-07],\n",
      "          [ 1.0571e-09, -1.1061e-09, -1.4373e-07,  ..., -1.3245e-07,\n",
      "            6.3765e-08, -9.4828e-08],\n",
      "          [-7.8248e-10,  5.9934e-09,  4.5084e-08,  ..., -3.6475e-07,\n",
      "           -2.7010e-08,  7.8638e-08],\n",
      "          ...,\n",
      "          [-1.7416e-07,  1.3448e-08, -3.1253e-07,  ...,  3.0752e-07,\n",
      "            2.5759e-07,  2.0211e-07],\n",
      "          [-9.0253e-08, -9.3044e-08,  1.2621e-07,  ...,  5.6283e-09,\n",
      "            1.2416e-07,  8.4518e-08],\n",
      "          [-2.6706e-08,  3.8280e-09, -5.3733e-08,  ...,  5.7811e-08,\n",
      "            1.5094e-07, -8.4956e-09]],\n",
      "\n",
      "         [[ 4.7050e-10, -3.8931e-09, -1.2386e-07,  ...,  1.7696e-07,\n",
      "            1.2923e-08,  2.0366e-07],\n",
      "          [-2.3381e-09,  2.2436e-09,  3.9225e-08,  ...,  1.2582e-07,\n",
      "            8.8460e-08,  1.9681e-07],\n",
      "          [ 2.1164e-09, -1.0030e-09, -1.8004e-07,  ..., -5.8333e-07,\n",
      "            2.5018e-07,  1.5124e-07],\n",
      "          ...,\n",
      "          [ 3.6494e-08,  1.4154e-08, -1.1453e-07,  ..., -2.2775e-07,\n",
      "           -9.4052e-08,  3.8062e-07],\n",
      "          [ 1.1102e-08,  6.1598e-08,  1.0391e-07,  ..., -1.1181e-07,\n",
      "           -2.0462e-07, -2.4975e-07],\n",
      "          [ 5.6714e-08, -4.3668e-08, -3.5131e-08,  ..., -1.9746e-07,\n",
      "            8.7483e-08,  1.1745e-07]]],\n",
      "\n",
      "\n",
      "        [[[ 8.5624e-09, -2.0033e-08, -2.3606e-07,  ...,  8.8829e-09,\n",
      "           -1.6396e-08,  0.0000e+00],\n",
      "          [ 5.2922e-08,  5.6245e-09, -1.3772e-07,  ..., -3.2786e-07,\n",
      "            1.6854e-07,  0.0000e+00],\n",
      "          [-8.1031e-08,  8.4708e-08, -9.1478e-09,  ..., -4.1813e-07,\n",
      "           -1.0872e-07, -4.2326e-08],\n",
      "          ...,\n",
      "          [ 2.3290e-09,  9.5839e-08, -3.3296e-07,  ..., -6.3986e-09,\n",
      "           -3.4316e-09,  1.9593e-08],\n",
      "          [-7.0099e-08,  2.5316e-07, -1.7700e-07,  ..., -9.6368e-09,\n",
      "            8.2546e-10, -1.4282e-08],\n",
      "          [-1.8200e-07, -3.4791e-08,  3.6358e-07,  ...,  8.7026e-11,\n",
      "           -1.9580e-08,  1.6538e-08]],\n",
      "\n",
      "         [[ 2.1985e-08, -7.5504e-09,  7.0462e-08,  ...,  6.2119e-10,\n",
      "           -2.3911e-08,  0.0000e+00],\n",
      "          [-2.0564e-08,  8.6543e-09, -1.4235e-07,  ..., -3.6726e-07,\n",
      "            1.1098e-07,  0.0000e+00],\n",
      "          [ 3.5171e-08,  9.1296e-08, -9.8984e-08,  ..., -1.5712e-07,\n",
      "            4.3984e-07,  5.4199e-08],\n",
      "          ...,\n",
      "          [ 1.4956e-08, -6.7887e-08, -3.7767e-07,  ..., -2.8989e-08,\n",
      "            2.3572e-08,  6.4475e-09],\n",
      "          [ 2.8584e-07,  2.8136e-07, -2.4506e-07,  ...,  1.8087e-09,\n",
      "           -2.2266e-08,  1.6385e-09],\n",
      "          [-1.5136e-07, -1.6275e-07,  1.8207e-07,  ...,  1.5548e-08,\n",
      "            1.1320e-08, -2.1018e-08]],\n",
      "\n",
      "         [[-1.8883e-08, -2.5841e-08,  4.3282e-10,  ...,  6.2646e-08,\n",
      "           -3.9926e-08,  0.0000e+00],\n",
      "          [-1.5830e-08,  7.3127e-09,  1.4490e-07,  ..., -6.9258e-08,\n",
      "           -2.2875e-08,  0.0000e+00],\n",
      "          [-1.2192e-08,  1.6971e-07, -4.2003e-07,  ..., -2.6394e-07,\n",
      "            5.0944e-07,  4.8573e-08],\n",
      "          ...,\n",
      "          [ 2.5860e-07, -4.8461e-09, -1.1620e-07,  ..., -1.5650e-09,\n",
      "            4.3264e-09, -7.4829e-09],\n",
      "          [ 2.5713e-07, -3.6121e-08, -4.2629e-07,  ..., -1.0642e-08,\n",
      "            3.4617e-10, -2.0906e-08],\n",
      "          [ 1.0610e-08,  3.3582e-07, -2.2254e-08,  ...,  9.7875e-09,\n",
      "           -1.9786e-08, -1.2624e-08]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-8.4701e-08, -1.7760e-07,  3.1107e-07,  ...,  5.4519e-09,\n",
      "            9.2244e-08,  0.0000e+00],\n",
      "          [ 9.9532e-08, -8.4473e-08, -8.2910e-08,  ...,  2.0907e-07,\n",
      "            3.9764e-10,  1.0465e-08],\n",
      "          [ 4.8893e-08,  2.6354e-08, -4.2469e-07,  ...,  1.2218e-07,\n",
      "            6.7619e-08,  3.1711e-07],\n",
      "          ...,\n",
      "          [ 7.6559e-08,  5.3052e-08,  4.1142e-08,  ..., -1.4125e-07,\n",
      "            5.4486e-08, -8.7007e-08],\n",
      "          [-3.5546e-08,  8.3724e-08, -5.1079e-08,  ..., -8.9338e-08,\n",
      "           -2.3470e-08,  3.4197e-08],\n",
      "          [ 0.0000e+00, -7.5077e-08, -1.4064e-07,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-5.2615e-08,  1.5838e-07,  8.8254e-08,  ...,  2.0945e-08,\n",
      "           -3.2125e-08,  0.0000e+00],\n",
      "          [ 1.1153e-07, -1.1394e-07, -3.5086e-08,  ..., -5.7713e-08,\n",
      "           -7.8490e-08,  4.3486e-08],\n",
      "          [-1.3511e-08,  1.4330e-08, -3.4310e-07,  ...,  3.2113e-08,\n",
      "            8.1503e-09, -1.3189e-07],\n",
      "          ...,\n",
      "          [ 1.4658e-07,  2.1715e-07,  2.8817e-07,  ..., -8.6104e-08,\n",
      "           -2.4862e-08,  3.2164e-08],\n",
      "          [ 3.3024e-08,  8.0538e-08,  1.3592e-07,  ...,  1.2297e-07,\n",
      "            4.4414e-09, -4.1248e-08],\n",
      "          [ 0.0000e+00,  9.2895e-08, -2.2922e-08,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-2.7565e-08,  1.6399e-08, -2.4448e-09,  ...,  5.3298e-08,\n",
      "           -5.3290e-08,  0.0000e+00],\n",
      "          [-9.4183e-08, -2.7718e-08, -1.3068e-07,  ...,  7.5957e-08,\n",
      "            5.6050e-08,  7.0518e-08],\n",
      "          [-1.4885e-09,  4.9939e-08,  1.6374e-07,  ..., -1.9922e-09,\n",
      "            2.5090e-07, -1.4066e-07],\n",
      "          ...,\n",
      "          [-9.4155e-08, -1.4251e-07,  2.5196e-07,  ...,  7.8298e-08,\n",
      "            2.0974e-08, -3.4351e-08],\n",
      "          [-7.0130e-08,  1.0257e-07, -1.4821e-07,  ...,  1.1509e-07,\n",
      "           -5.1727e-08, -2.4129e-08],\n",
      "          [ 0.0000e+00,  7.3035e-08, -1.9495e-07,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.1405e-07,  8.6815e-08,  2.1946e-07,  ..., -4.6086e-07,\n",
      "            1.2983e-08, -1.7279e-07],\n",
      "          [-1.8789e-08, -7.5199e-08, -2.7576e-07,  ...,  3.7695e-07,\n",
      "           -2.5362e-07, -2.0717e-08],\n",
      "          [ 2.1174e-07, -7.1742e-08,  6.5989e-08,  ...,  8.7160e-08,\n",
      "           -9.2979e-08, -1.3552e-07],\n",
      "          ...,\n",
      "          [ 4.4828e-07,  8.0608e-08, -3.6556e-07,  ...,  1.1974e-07,\n",
      "           -1.6190e-07, -1.0821e-07],\n",
      "          [ 3.4109e-07,  7.7197e-07,  4.4026e-07,  ...,  3.3496e-07,\n",
      "           -1.8769e-07,  3.4821e-07],\n",
      "          [-1.0776e-07, -1.1428e-07, -4.5835e-07,  ..., -6.6597e-08,\n",
      "            2.4506e-07, -2.2783e-07]],\n",
      "\n",
      "         [[ 2.4450e-07,  3.8341e-08, -2.2171e-07,  ...,  1.6073e-07,\n",
      "           -8.5390e-08,  1.0424e-07],\n",
      "          [-1.2047e-07, -2.1076e-07,  1.7101e-07,  ..., -5.6362e-07,\n",
      "            1.5748e-08, -7.6907e-08],\n",
      "          [-2.7694e-07,  1.0166e-07,  6.9715e-07,  ...,  2.5171e-07,\n",
      "           -4.2824e-08, -5.1262e-09],\n",
      "          ...,\n",
      "          [ 2.7616e-07,  6.7052e-07, -8.6671e-07,  ...,  1.2281e-06,\n",
      "           -1.9878e-07, -1.3447e-07],\n",
      "          [-2.8110e-07, -2.1619e-07,  6.8362e-07,  ..., -1.9562e-07,\n",
      "            1.6125e-07, -1.2534e-07],\n",
      "          [-8.1584e-08,  1.0331e-07,  3.5382e-07,  ..., -1.2973e-07,\n",
      "           -1.1697e-07,  2.5051e-07]],\n",
      "\n",
      "         [[-1.5539e-07, -8.5723e-08,  2.0284e-07,  ...,  2.4285e-07,\n",
      "           -2.0911e-07,  1.7330e-07],\n",
      "          [-5.6585e-08, -2.1519e-07,  3.0672e-08,  ...,  8.3743e-08,\n",
      "           -1.7216e-07, -5.0459e-08],\n",
      "          [-8.1014e-08,  2.4344e-07,  1.0914e-07,  ..., -2.5693e-07,\n",
      "            1.3332e-07, -2.4585e-08],\n",
      "          ...,\n",
      "          [-2.2827e-07, -7.1851e-07,  3.0285e-07,  ...,  3.0258e-07,\n",
      "           -3.4754e-07, -2.4795e-07],\n",
      "          [ 6.2700e-07, -9.1653e-08, -1.1176e-07,  ..., -7.4705e-08,\n",
      "            8.0498e-08,  1.3646e-07],\n",
      "          [ 2.6379e-07, -1.5234e-07, -2.2699e-07,  ..., -7.1084e-08,\n",
      "            2.3407e-07,  1.3907e-07]]],\n",
      "\n",
      "\n",
      "        [[[-4.6743e-08,  2.4160e-07,  2.9848e-07,  ..., -3.0288e-08,\n",
      "            3.2972e-07,  1.2220e-07],\n",
      "          [ 5.7971e-08,  1.3479e-07, -3.4826e-07,  ...,  1.2698e-07,\n",
      "           -1.9873e-07, -4.5764e-08],\n",
      "          [-5.9282e-07, -6.2411e-07, -8.8852e-07,  ...,  3.0254e-07,\n",
      "           -4.0070e-07,  8.6849e-09],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  3.7244e-07,  2.1631e-07,  ..., -2.4481e-07,\n",
      "           -2.3521e-07, -1.5788e-07],\n",
      "          [ 0.0000e+00,  1.2039e-07, -1.4834e-07,  ...,  8.9981e-09,\n",
      "           -5.6721e-08, -1.3013e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-1.0562e-07,  5.8900e-07,  2.7056e-07,  ...,  1.2841e-07,\n",
      "           -1.0923e-07, -1.8361e-07],\n",
      "          [ 2.0276e-08,  1.5485e-07, -5.2668e-07,  ..., -7.7185e-07,\n",
      "           -3.2976e-07,  1.2494e-07],\n",
      "          [-2.7655e-07, -9.7777e-07,  1.1164e-07,  ...,  8.5325e-07,\n",
      "            7.8926e-07, -1.0361e-07],\n",
      "          ...,\n",
      "          [ 0.0000e+00, -1.5281e-07, -1.9714e-07,  ...,  5.4910e-08,\n",
      "            1.7637e-08, -1.0615e-07],\n",
      "          [ 0.0000e+00, -1.4842e-08,  1.5077e-07,  ..., -1.9897e-07,\n",
      "           -4.0762e-08,  1.5027e-08],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 4.0100e-08,  2.5516e-07, -1.1495e-07,  ..., -3.7966e-07,\n",
      "           -1.7654e-07, -2.6833e-07],\n",
      "          [ 6.7596e-08,  1.3179e-07, -3.8357e-07,  ...,  1.1783e-07,\n",
      "           -4.4457e-07, -2.5930e-07],\n",
      "          [-2.0231e-07, -7.3795e-07,  1.2601e-07,  ...,  6.0892e-07,\n",
      "            7.8472e-08, -1.9927e-07],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  5.3567e-07, -6.3326e-08,  ...,  1.1414e-07,\n",
      "            3.5272e-07,  2.8230e-07],\n",
      "          [ 0.0000e+00,  1.7192e-07, -2.8173e-07,  ...,  1.9708e-07,\n",
      "            1.4537e-07, -9.3593e-08],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in disc_new.named_parameters():\n",
    "    print(name, param.grad)\n",
    "\n",
    "print(jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 32, 32])\n",
      "tensor(99.9980, grad_fn=<AddBackward0>)\n",
      "conv.weight tensor([[[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]]])\n",
      "conv.bias tensor([nan, nan, nan])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of target should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[7], line 74\u001b[0m\n",
      "\u001b[1;32m     72\u001b[0m generated_new \u001b[38;5;241m=\u001b[39m apt(jacobian\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[1;32m     73\u001b[0m lreal \u001b[38;5;241m=\u001b[39m bce(real, disc(x1))\n",
      "\u001b[0;32m---> 74\u001b[0m lfake \u001b[38;5;241m=\u001b[39m \u001b[43mbce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     75\u001b[0m l_adv \u001b[38;5;241m=\u001b[39m lreal \u001b[38;5;241m+\u001b[39m lfake\n",
      "\u001b[1;32m     76\u001b[0m l_phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39ml_adv\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n",
      "\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3122\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n",
      "\u001b[1;32m   3119\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;32m   3120\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n",
      "\u001b[0;32m-> 3122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of target should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model = CNN().to(device)\n",
    "apt = Adaptor().to(device)\n",
    "disc = Discriminator().to(device)\n",
    "\n",
    "\n",
    "\n",
    "for data in testloader:\n",
    "    x, y = data\n",
    "    break\n",
    "\n",
    "x1 = x.clone()\n",
    "y1 = y.clone()\n",
    "a = 0.0001\n",
    "u = 1\n",
    "model1 = model\n",
    "apt1 = apt\n",
    "disc_new =disc\n",
    "opt_model = torch.optim.Adam(model.parameters(), lr=a)\n",
    "opt_apt = torch.optim.Adam(apt.parameters(), lr=a)\n",
    "opt_disc = torch.optim.Adam(disc.parameters(), lr=a)\n",
    "\n",
    "model1.train()\n",
    "model.train\n",
    "apt.train()\n",
    "disc.train()\n",
    "\n",
    "\n",
    "x.requires_grad = True\n",
    "output = model(x)\n",
    "x.retain_grad()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "model.zero_grad()\n",
    "l = loss(output,y)\n",
    "l.backward(retain_graph=True)\n",
    "jacobian = x.grad\n",
    "x.requires_grad = False\n",
    "print(np.shape(jacobian))\n",
    "generated = apt1(jacobian.detach())\n",
    "#CALCOLO LOSS ADVERSARIAL SFRUTTANDO DEFINIZIONE DI BCE\n",
    "\n",
    "real = torch.ones_like(disc(x1))\n",
    "fake = torch.zeros_like(disc(generated))\n",
    "bce = nn.BCELoss()\n",
    "#\n",
    "lreal = bce(real, disc(x1))\n",
    "lfake = bce(fake, disc(generated))\n",
    "l_adv = lreal + lfake\n",
    "print(l_adv)\n",
    "#CALCOLO LOSS DI TRAINING, NOMI LOSS ASSEGNATI SECONDO I PARAMETRI DEL PAPER\n",
    "l_theta = l + u*l_adv   #theta parametro di model \n",
    "\n",
    "opt_model.zero_grad()\n",
    "opt_apt.zero_grad()\n",
    "l_theta.backward(retain_graph=True)\n",
    "opt_model.step()\n",
    "opt_apt.step()\n",
    "\n",
    "#loss1 = nn.CrossEntropyLoss()\n",
    "#output1 = model(x1)\n",
    "#l1 = loss1(output1,y1)\n",
    "\n",
    "#opt_apt.zero_grad()\n",
    "#l_adv.backward()\n",
    "for name, param in apt.named_parameters():\n",
    "    print(name, param.grad)\n",
    "#opt_apt.step()\n",
    "#\n",
    "##Aggiorno il parametro di disc, ma prima calcolo il valore di generated con i pesi di adaptor aggiornati\n",
    "#\n",
    "generated_new = apt(jacobian.detach())\n",
    "lreal = bce(real, disc(x1))\n",
    "lfake = bce(fake, disc(generated_new.detach()))\n",
    "l_adv = lreal + lfake\n",
    "l_phi = -1*l_adv\n",
    "\n",
    "opt_disc.zero_grad()\n",
    "l_phi.backward()\n",
    "opt_disc.step()\n",
    "\n",
    "for name, param in disc.named_parameters():\n",
    "    print(name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]],\n",
      "\n",
      "\n",
      "        [[[nan]],\n",
      "\n",
      "         [[nan]],\n",
      "\n",
      "         [[nan]]]])\n"
     ]
    }
   ],
   "source": [
    "#apt = Adaptor().to('cpu')\n",
    "#apt1=apt\n",
    "#apt.train\n",
    "bce = nn.BCELoss()\n",
    "#print(x)\n",
    "#for name, param in apt.named_parameters():\n",
    "#    if param.requires_grad:\n",
    "#        print(name, param.data)\n",
    "#print(type(apt(x)[0][0][0][0].item()))\n",
    "print(apt.conv.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_Jarn(dataloader, epochs, model, apt, disc, a, b, c, u):\n",
    "    opt_model = torch.optim.Adam(model.parameters(), lr=a)\n",
    "    opt_apt = torch.optim.Adam(apt.parameters(), lr=b)\n",
    "    opt_disc = torch.optim.Adam(disc.parameters(), lr=c)\n",
    "\n",
    "    model.train()\n",
    "    apt.train()\n",
    "    disc.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_l_adv = 0\n",
    "        running_loss = 0\n",
    "        for data in dataloader:\n",
    "            x,y = data\n",
    "            x.requires_grad = True\n",
    "            output = model(x)\n",
    "            x.retain_grad()\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "\n",
    "            model.zero_grad()\n",
    "            l = loss(output,y)\n",
    "            l.backward(retain_graph = True)\n",
    "            jacobian = x.grad\n",
    "            x.requires_grad = False\n",
    "            print(np.shape(jacobian))\n",
    "            generated = apt(jacobian)\n",
    "\n",
    "            #CALCOLO LOSS ADVERSARIAL SFRUTTANDO DEFINIZIONE DI BCE\n",
    "            real = torch.ones_like(disc(x))\n",
    "            fake = torch.zeros_like(disc(jacobian))\n",
    "            bce = nn.BCELoss()\n",
    "            \n",
    "            lreal = bce(real, disc(x))\n",
    "            lfake = bce(fake, disc(generated))\n",
    "            l_adv = lreal + lfake\n",
    "\n",
    "            running_l_adv += l_adv.item()\n",
    "\n",
    "            #CALCOLO LOSS DI TRAINING, NOMI LOSS ASSEGNATI SECONDO I PARAMETRI DEL PAPER\n",
    "\n",
    "            l_theta = l + u*l_adv   #theta parametro di model \n",
    "                                    #psi parametro di apt, da aggiornare minimizzando l_adv\n",
    "            l_phi = -1*l_adv        #phi parametro di disc, da aggiornare massimizzando l_adv\n",
    "            \n",
    "\n",
    "            #Aggiorno parametri\n",
    "            \n",
    "            opt_model.zero_grad()\n",
    "            l_theta.backward(retain_graph= True)\n",
    "            opt_model.step()\n",
    "             \n",
    "            opt_apt.zero_grad()\n",
    "            l_adv.backward(retain_graph= True)\n",
    "            opt_apt.step()\n",
    "\n",
    "            opt_disc.zero_grad()\n",
    "            l_phi.backward()\n",
    "            opt_disc.step()\n",
    "            \n",
    "            \n",
    "        print(epoch, running_loss/len(dataloader),running_l_adv/len(dataloader))\n",
    "    torch.save(model.state_dict(), './JARN_PARAMS.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 3, 1, 1]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m epoche\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[43mtraining_Jarn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoche\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[16], line 60\u001b[0m, in \u001b[0;36mtraining_Jarn\u001b[0;34m(dataloader, epochs, model, apt, disc, a, b, c, u)\u001b[0m\n",
      "\u001b[1;32m     57\u001b[0m     opt_apt\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;32m     59\u001b[0m     opt_disc\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;32m---> 60\u001b[0m     \u001b[43ml_phi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     61\u001b[0m     opt_disc\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch, running_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader),running_l_adv\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader))\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[1;32m    491\u001b[0m     )\n",
      "\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n",
      "\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 3, 1, 1]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "epoche= 5\n",
    "training_Jarn(trainloader, epoche, model, apt,  disc, a= 0.0001, b=0.0001,c = 0.0001, u =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Exercise 3.3: Experiment with *targeted* adversarial attacks\n",
    "Implement the targeted Fast Gradient Sign Method to generate adversarial samples that *imitate* samples from a specific class. Evaluate your adversarial samples qualitatively and quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
