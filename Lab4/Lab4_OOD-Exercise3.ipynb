{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Wildcard\n",
    "\n",
    "You know the drill. Pick *ONE* of the following exercises to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import FakeData\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Exercise 3.1: Implement ODIN for OOD detection\n",
    "ODIN is a very simple approach, and you can already start experimenting by implementing a temperature hyperparameter in your base model and doing a grid search on $T$ and $\\varepsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple CNN model.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "dim_fake = 1000\n",
    "img_size = (3, 32, 32)\n",
    "\n",
    "ds_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "ds_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "ds_fake = FakeData(size=dim_fake, image_size=img_size, transform=transform)\n",
    "dl_fake = torch.utils.data.DataLoader(ds_fake, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# PRETRAINED MODEL.\n",
    "model.load_state_dict(torch.load('./cifar10_CNN.pth')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(data_loader, score_fun):\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            x, y = data\n",
    "            output = model(x.to(device))\n",
    "            s = score_fun(output)\n",
    "            scores.append(s)\n",
    "        scores_t = torch.cat(scores)\n",
    "        return scores_t\n",
    "\n",
    "\n",
    "def detection(model, data_loader, T, eps, delta):\n",
    "    model.train()\n",
    "\n",
    "    score = []\n",
    "    for (xs, ys) in data_loader:\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "        xs.requires_grad=True \n",
    "        xs.retain_grad()\n",
    "\n",
    "        output = model(xs)\n",
    "        model.zero_grad()\n",
    "        l = F.cross_entropy(output, ys)\n",
    "        l.backward()\n",
    "\n",
    "        # FGSM\n",
    "        v = xs + eps*torch.sign(xs.grad) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = model(v)\n",
    "            # Get the max softmax score\n",
    "            s = F.softmax(logit / T, 1).max(dim=1)[0]\n",
    "\n",
    "            # Compute the score \n",
    "            for i in range(np.shape(s)[0]): \n",
    "                if s[i] > delta:\n",
    "                    score.append(1)\n",
    "                else:\n",
    "                    score.append(0)\n",
    "    score_t = torch.tensor(score)\n",
    "    return score_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH\n",
    "# 10 values for epsilon, 7 values for temperature\n",
    "e = np.linspace(0, 0.004, 10)\n",
    "t = np.array([1, 5, 10, 50, 100, 500, 1000])\n",
    "\n",
    "for eps in e:\n",
    "    for temp in t:\n",
    "        scores_fake= detection(model, dl_fake, temp, eps, delta=0.5)\n",
    "        scores_test= detection(model, dl_test, temp, eps, delta=0.5)\n",
    "        y_test = torch.ones_like(scores_test)\n",
    "        y_fake = torch.zeros_like(scores_fake)\n",
    "        y = torch.cat((y_test, y_fake))\n",
    "        y_pred = torch.cat((scores_test, scores_fake))\n",
    "        s = metrics.roc_auc_score(y, y_pred)\n",
    "        print(\"AUC for eps = %s, temp = %s is %s\" % (eps, temp, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Exercise 3.2: Implement JARN\n",
    "In exercise 2.2 you already implemented Jacobian-regularized learning to make your model more robust to adversarial samples. Add a *discriminator* to your model to encourage the adversarial samples used for regularization to be more *salient*.\n",
    "\n",
    "See [the JARN paper](https://arxiv.org/abs/1912.10185) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Adaptor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3,3,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.tanh(x)\n",
    "        return x\n",
    "     \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.MaxPool2d(2,2)\n",
    "            )\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(64*10*10, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x= torch.flatten(x,1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JARN\n",
    "def training_model(dataloader, epochs, model, apt, disc, lr_model, lr_apt, lr_disc, u):\n",
    "    model.train()\n",
    "    apt.train()\n",
    "    disc.train()\n",
    "\n",
    "    opt_model = torch.optim.Adam(model.parameters(), lr=lr_model)\n",
    "    opt_apt = torch.optim.Adam(apt.parameters(), lr=lr_apt)\n",
    "    opt_disc = torch.optim.Adam(disc.parameters(), lr=lr_disc)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tot_loss_avd = 0\n",
    "        tot_loss = 0\n",
    "        for (xs, ys) in tqdm(range(dataloader)):\n",
    "            xs, ys = xs.to(device), ys.to(device)\n",
    "            xs.requires_grad = True\n",
    "\n",
    "            output = model(xs)\n",
    "            xs.retain_grad()\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss = nn.CrossEntropyLoss(output, ys)\n",
    "            loss.backward(retain_graph = True)\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            jac = xs.grad\n",
    "            xs.requires_grad = False\n",
    "\n",
    "            generated = apt(jac)\n",
    "\n",
    "            # Adversarial loss with binary cross entropy\n",
    "            reals = torch.ones_like(disc(xs))\n",
    "            fakes = torch.zeros_like(disc(jac))\n",
    "\n",
    "            loss_reals = nn.BCELoss(reals, disc(xs))\n",
    "            loss_fake = nn.BCELoss(fakes, disc(generated))\n",
    "            loss_adv = loss_reals + loss_fake\n",
    "\n",
    "            tot_loss_avd += loss_adv.item()\n",
    "\n",
    "            # Training model loss\n",
    "            l_theta = loss + u*loss_adv   \n",
    "            l_phi = -1*loss_adv     \n",
    "\n",
    "            # Update the parameters\n",
    "            opt_model.zero_grad()\n",
    "            opt_apt.zero_grad()\n",
    "            opt_disc.zero_grad()\n",
    "\n",
    "            l_theta.backward(retain_graph= True)\n",
    "            loss_adv.backward(retain_graph= True)\n",
    "            l_phi.backward()\n",
    "\n",
    "            opt_model.step()\n",
    "            opt_apt.step()\n",
    "            opt_disc.step()\n",
    "            \n",
    "        print(epoch, tot_loss/len(dataloader),tot_loss_avd/len(dataloader))\n",
    "    torch.save(model.state_dict(), './model_states/jarn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "ds_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "ds_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = CNN().to(device)\n",
    "apt = Adaptor().to(device)\n",
    "disc = Discriminator().to(device)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "lr_model = 0.0001\n",
    "lr_apt = 0.0001\n",
    "lr_disc = 0.0001\n",
    "u=1\n",
    "\n",
    "training_model(dl_train, epochs, model, apt, disc, lr_model, lr_apt, lr_disc, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Exercise 3.3: Experiment with *targeted* adversarial attacks\n",
    "Implement the targeted Fast Gradient Sign Method to generate adversarial samples that *imitate* samples from a specific class. Evaluate your adversarial samples qualitatively and quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
